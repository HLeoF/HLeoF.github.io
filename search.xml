<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Support Vector Machine (SVM) Conclusion</title>
      <link href="/2024/04/15/svm-conclu/"/>
      <url>/2024/04/15/svm-conclu/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Support Vector Machine </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Conclusion </tag>
            
            <tag> Decision Trees </tag>
            
            <tag> Supervised Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Support Vector Machine (SVM) Result</title>
      <link href="/2024/04/15/svm-result/"/>
      <url>/2024/04/15/svm-result/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Support Vector Machine </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Results </tag>
            
            <tag> Supervised Machine Learning </tag>
            
            <tag> Support Vector Machine </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Support Vector Machine (SVM) Code</title>
      <link href="/2024/04/15/svm-code/"/>
      <url>/2024/04/15/svm-code/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Support Vector Machine </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Code </tag>
            
            <tag> Supervised Machine Learning </tag>
            
            <tag> Support Vector Machine </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Support Vector Machine (SVM) Data Preparation</title>
      <link href="/2024/04/15/svm-data/"/>
      <url>/2024/04/15/svm-data/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Support Vector Machine </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Preparation </tag>
            
            <tag> Supervised Machine Learning </tag>
            
            <tag> Support Vector Machine </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Support Vector Machine (SVM) Overview</title>
      <link href="/2024/04/15/svm-intro/"/>
      <url>/2024/04/15/svm-intro/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Support Vector Machine </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Overview </tag>
            
            <tag> Supervised Machine Learning </tag>
            
            <tag> Support Vector Machine </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decision Tree Conclusion</title>
      <link href="/2024/03/22/dt-conclu/"/>
      <url>/2024/03/22/dt-conclu/</url>
      
        <content type="html"><![CDATA[<p><img src="/DT/conclu/1.jpeg" alt="Barcelona Defensive Player 2022 Season"></p><p>The performance of defensive players often has a key impact on the team’s performance. A solid defense is more important than offense from the forwards which is the main trend in modern soccer. As Barcelona showed last season, even though the team may not be as capable going forward as some other clubs, they still won the La Liga title due to their excellent defense. Great defenders are important to make up for deficiencies on the attacking end and contribute the to team’s ranking.</p><hr><h1 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h1><p>img: <a href="https://images.app.goo.gl/MSM2vwEKiC9uhxKFA">https://images.app.goo.gl/MSM2vwEKiC9uhxKFA</a></p>]]></content>
      
      
      <categories>
          
          <category> Decision Tree </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Conclusion </tag>
            
            <tag> Decision Trees </tag>
            
            <tag> Supervised Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decision Tree Results</title>
      <link href="/2024/03/22/dt-result/"/>
      <url>/2024/03/22/dt-result/</url>
      
        <content type="html"><![CDATA[<h1 id="Accuracy-of-Decision-Trees-Prediction"><a href="#Accuracy-of-Decision-Trees-Prediction" class="headerlink" title="Accuracy of Decision Trees Prediction"></a>Accuracy of Decision Trees Prediction</h1><h2 id="Decision-Tree-Model-1-Gini-Better-Splitter"><a href="#Decision-Tree-Model-1-Gini-Better-Splitter" class="headerlink" title="Decision Tree Model 1 : Gini &amp; Better Splitter"></a>Decision Tree Model 1 : Gini &amp; Better Splitter</h2><p>In the first model, the main changed parameters are <code>criterion=gini</code> and <code>splitter=better</code>. Here, <code>splitter=better</code> means the best splitting method. At each tree node, the algorithm tries all combinations of features and eigenvalues. Then it selects the segmentation that allows the most improvement in the purity of the leaf nodes. The final tree plot is shown below. </p><p><img src="/DT/result/1.jpg" alt="Tree 1"> </p><p>This following confusion matrix can evaluate the prediction preformance of this model. According to the fomula of accuracy.</p><p>$$\frac{\text{Number of corretly classified Samples}}{\text{Total Number of Samples}} &#x3D; \frac{19+13+25}{19+4+0+1+13+2+3+0+25} \approx 0.85$$</p><pre class="language-Python" data-language="Python"><code class="language-Python">from sklearn.metrics import accuracy_scoreaccuracy &#x3D; accuracy_score(playerTestLabel, predictions)print(accuracy)&gt;&gt;&gt; 0.8507462686567164</code></pre><p><img src="/DT/result/2.png" alt="Tree 1 Confusion Matrix"></p><p>Therefore, this model is about <code>85%</code> accurate in predicting categories. And getting the precision rate for each category by using the confusion matrix:</p><p>$$\text{‘Good’ Precision} &#x3D; \frac{19}{19+1+3} \approx 0.83$$</p><p>$$\text{‘Normal’ Precision} &#x3D; \frac{13}{4+13+0} \approx 0.77$$</p><p>$$\text{‘Bad’ Precision} &#x3D; \frac{25}{0+2+25}  \approx 0.93$$</p><p>Based on these precision rates can be seen that category of <code>Bad</code> has highest precision rate, which is about 93%. The category of <code>Normal</code> has lowest precision rate of about 77%. Thus, predicting the category as <code>Bad</code> is the most accurate, and predicting the category as <code>Normal</code> is less effective than other two categories.</p><h2 id="Decision-Tree-Model-2-Entropy-Better-Splitter"><a href="#Decision-Tree-Model-2-Entropy-Better-Splitter" class="headerlink" title="Decision Tree Model 2: Entropy &amp; Better Splitter"></a>Decision Tree Model 2: Entropy &amp; Better Splitter</h2><p>In the second model, the main changed parameter is <code>criterion=Entropy</code> The second tree plot is shown below. </p><p><img src="/DT/result/3.jpg" alt="Tree 2"></p><p>This following confusion matrix can evaluate the prediction preformance of this model. According to the fomula of accuracy.</p><p>$$\frac{\text{Number of corretly classified Samples}}{\text{Total Number of Samples}} &#x3D; \frac{21+9+25}{21+2+0+5+9+2+3+0+25} \approx 0.82$$</p><pre class="language-Python" data-language="Python"><code class="language-Python">from sklearn.metrics import accuracy_scoreaccuracy2 &#x3D; accuracy_score(playerTestLabel, predictions2)print(accuracy2)&gt;&gt;&gt; 0.8208955223880597</code></pre><p><img src="/DT/result/4.png" alt="Tree 2 Confusion Matrix"></p><p>Therefore, this model is about <code>82%</code> accurate in predicting categories. And getting the precision rate for each category by using the confusion matrix:</p><p>$$\text{‘Good’ Precision} &#x3D; \frac{21}{21+5+3} \approx 0.72$$</p><p>$$\text{‘Normal’ Precision} &#x3D; \frac{9}{2+9+0} \approx 0.81$$</p><p>$$\text{‘Bad’ Precision} &#x3D; \frac{25}{0+2+25}  \approx 0.93$$</p><p>Thus, according to these precision rates can be seen that category of <code>Bad</code> has highest precision rate, which is about 93%. The category of <code>Good</code> has lowest precision rate of about 77%. Thus, predicting the category as <code>Bad</code> is the most accurate, and predicting the category as <code>Good</code> is less effective than other two categories.</p><h2 id="Decision-Tree-Model-3-Gini-Random-Splitter"><a href="#Decision-Tree-Model-3-Gini-Random-Splitter" class="headerlink" title="Decision Tree Model 3: Gini &amp; Random Splitter"></a>Decision Tree Model 3: Gini &amp; Random Splitter</h2><p>In the third model, the main changed parameter is <code>splitter=random</code>. Here, <code>splitter=random</code> means the best splitting method. At each tree node, the algorithm random select a feature and a eigenvalue. It leads to greater randomness in the generated decision tree. The third tree plot is shown below. </p><p><img src="/DT/result/5.jpg" alt="Tree 3"></p><p>This following confusion matrix can evaluate the prediction preformance of this model. According to the fomula of accuracy.</p><p>$$\frac{\text{Number of corretly classified Samples}}{\text{Total Number of Samples}} &#x3D; \frac{20+16+25}{20+3+0+0+16+0+3+0+25} \approx 0.91$$</p><pre class="language-Python" data-language="Python"><code class="language-Python">from sklearn.metrics import accuracy_scoreaccuracy3 &#x3D; accuracy_score(playerTestLabel, predictions3)print(accuracy3)&gt;&gt;&gt; 0.9104477611940298</code></pre><p><img src="/DT/result/6.png" alt="Tree 3 Confusion Matrix"></p><p>Therefore, this model is about <code>91%</code> accurate in predicting categories. And getting the precision rate for each category by using the confusion matrix:</p><p>$$\text{‘Good’ Precision} &#x3D; \frac{20}{20+0+3} \approx 0.87$$</p><p>$$\text{‘Normal’ Precision} &#x3D; \frac{16}{3+16+0} \approx 0.84$$</p><p>$$\text{‘Bad’ Precision} &#x3D; \frac{25}{0+0+25}  \approx 1.00$$</p><p>Therefore, according to these precision rates can be seen that category of <code>Bad</code> has highest precision rate, which is about 100%. The category of <code>Normal</code> has lowest precision rate of about 84%. Thus, predicting the category as <code>Bad</code> is the most accurate, and predicting the category as <code>Normal</code> is less effective than other two categories.</p><h1 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h1><p><img src="/DT/result/7.jpg" alt="Prediction Accuracy of Models Comparison"></p><p>From the bar plot, the <a href="https://hleof.github.io/2024/03/22/dt-result/">model 3</a> has the best prediction affection in the decision tree models.  It is about <code>91%</code> accurate. Using this model to show performance prediction for defenders (show shown below).</p><p><img src="/DT/result/8.jpg" alt="Defenders Prediction Results"></p><p>The model’s predictions are good. For example, Sevilla defender J.Kounde. During the 2020 season, he was in great form and attracted the attention of Chelsea and FC Barcelona. Eventually, he chose to join Barcelona. At FC Barcelona, he was highly regarded and won the La Liga title for the club. Also, Atletico Madrid defender J.Gimenez shined in the 2020 season, winning the La Liga title for his club in 2020. Therefore, these predictions are realistic. </p><p><a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/DT/Predition_defender_results.csv">Defenders Prediction Dataset</a></p>]]></content>
      
      
      <categories>
          
          <category> Decision Tree </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Results </tag>
            
            <tag> Decision Trees </tag>
            
            <tag> Supervised Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decision Tree Code</title>
      <link href="/2024/03/22/dt-code/"/>
      <url>/2024/03/22/dt-code/</url>
      
        <content type="html"><![CDATA[<h1 id="Decision-Tree-Algorithm-Python-Code"><a href="#Decision-Tree-Algorithm-Python-Code" class="headerlink" title="Decision Tree Algorithm Python Code"></a>Decision Tree Algorithm Python Code</h1><p>Data Perparation Code <a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/DT/DT_pre_data.py">Python Code</a></p><p>The Train Dataset <a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/DT/DTTrainDF.csv">Train Set</a></p><p>The Test Dataset <a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/DT/DTTestDF.csv">Test Set</a></p><p>Decision Tree Algorithm <a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/DT/decision_tree_model.py">Python</a></p><pre class="language-Python" data-language="Python"><code class="language-Python">import pandas as pdfrom sklearn import treeimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.metrics import confusion_matrixfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_scoreimport graphvizplayerTrainDF &#x3D; pd.read_csv(&quot;DTTrainDF.csv&quot;)playerTestDF &#x3D; pd.read_csv(&quot;DTTestDF.csv&quot;)playerTestLabel &#x3D; playerTestDF[&#39;performance&#39;]playerTestDF &#x3D; playerTestDF.drop([&#39;performance&#39;],axis &#x3D; 1)playerTrainDF_nolabel &#x3D; playerTrainDF.drop([&#39;performance&#39;],axis &#x3D; 1)playerTrainLabel &#x3D; playerTrainDF[&#39;performance&#39;]dropcols &#x3D; [&#39;season&#39;,&#39;teamName&#39;,&#39;playerName&#39;,&#39;position&#39;]playerTrainDF_nolabel_quant &#x3D; playerTrainDF_nolabel.drop(dropcols,axis &#x3D; 1)playerTestDF_quant &#x3D; playerTestDF.drop(dropcols,axis&#x3D;1)############ Tree 1: Gini &amp; Best ################################model &#x3D; DecisionTreeClassifier(criterion&#x3D;&#39;gini&#39;, splitter&#x3D;&#39;best&#39;, max_depth&#x3D;None,                  min_samples_split&#x3D;2, min_samples_leaf&#x3D;1,                  min_weight_fraction_leaf&#x3D;0.0,                  max_features&#x3D;None, random_state&#x3D;42,                  max_leaf_nodes&#x3D;None, min_impurity_decrease&#x3D;0.0,                  class_weight&#x3D;None, ccp_alpha&#x3D;0.0)model.fit(playerTrainDF_nolabel_quant,playerTrainLabel)TREE_Vis &#x3D; tree.export_graphviz(model,                    out_file&#x3D;None,                    feature_names&#x3D;playerTrainDF_nolabel_quant.columns,                    class_names&#x3D;[&quot;Good&quot;, &quot;Normal&quot;, &quot;Bad&quot;],                    filled&#x3D;True, rounded&#x3D;True,                    special_characters&#x3D;True)graph &#x3D; graphviz.Source(TREE_Vis)graphpredictions &#x3D; model.predict(playerTestDF_quant)conf_matrix &#x3D; confusion_matrix(playerTestLabel, predictions)accuracy &#x3D; accuracy_score(playerTestLabel, predictions)print(accuracy)labels &#x3D; [&#39;Good&#39;,&#39;Normal&#39;,&#39;Bad&#39;]sns.heatmap(conf_matrix, annot&#x3D;True,cmap&#x3D;&#39;Blues&#39;,xticklabels&#x3D;labels, yticklabels&#x3D;labels, cbar&#x3D;False)plt.title(&quot;Confusion Matrix&quot;,fontsize&#x3D;20)plt.xlabel(&quot;Actual&quot;, fontsize&#x3D;15)plt.ylabel(&quot;Predicted&quot;, fontsize&#x3D;15)plt.show()################## Tree plot 2 Entropy &amp; Best splitter ######################model2 &#x3D; DecisionTreeClassifier(criterion&#x3D;&#39;entropy&#39;,splitter&#x3D;&#39;best&#39;,max_depth&#x3D;None,                  min_samples_split&#x3D;2, min_samples_leaf&#x3D;1,                  min_weight_fraction_leaf&#x3D;0.0,                  max_features&#x3D;None, random_state&#x3D;42,                  max_leaf_nodes&#x3D;None, min_impurity_decrease&#x3D;0.0,                  class_weight&#x3D;None, ccp_alpha&#x3D;0.0)model2.fit(playerTrainDF_nolabel_quant,playerTrainLabel)TREE_Vis &#x3D; tree.export_graphviz(model2,                    out_file&#x3D;None,                    feature_names&#x3D;playerTrainDF_nolabel_quant.columns,                    class_names&#x3D;[&quot;Good&quot;, &quot;Normal&quot;, &quot;Bad&quot;],                    filled&#x3D;True, rounded&#x3D;True,                    special_characters&#x3D;True)graph &#x3D; graphviz.Source(TREE_Vis)graphpredictions2 &#x3D; model2.predict(playerTestDF_quant)conf_matrix2 &#x3D; confusion_matrix(playerTestLabel, predictions2)accuracy2 &#x3D; accuracy_score(playerTestLabel, predictions2)print(conf_matrix2)print(accuracy2)sns.heatmap(conf_matrix2, annot&#x3D;True,cmap&#x3D;&#39;Blues&#39;,xticklabels&#x3D;labels, yticklabels&#x3D;labels, cbar&#x3D;False)plt.title(&quot;Confusion Matrix&quot;,fontsize&#x3D;20)plt.xlabel(&quot;Actual&quot;, fontsize&#x3D;15)plt.ylabel(&quot;Predicted&quot;, fontsize&#x3D;15)plt.show()######################### Tree Plot 3 Gini and random splitter #########################model3 &#x3D; DecisionTreeClassifier(criterion&#x3D;&#39;gini&#39;,splitter&#x3D;&#39;random&#39;,max_depth&#x3D;None,                  min_samples_split&#x3D;2, min_samples_leaf&#x3D;1,                  min_weight_fraction_leaf&#x3D;0.0,                  max_features&#x3D;None, random_state&#x3D;42,                  max_leaf_nodes&#x3D;None, min_impurity_decrease&#x3D;0.0,                  class_weight&#x3D;None, ccp_alpha&#x3D;0.0)model3.fit(playerTrainDF_nolabel_quant,playerTrainLabel)TREE_Vis &#x3D; tree.export_graphviz(model3,                    out_file&#x3D;None,                    feature_names&#x3D;playerTrainDF_nolabel_quant.columns,                    class_names&#x3D;[&quot;Good&quot;, &quot;Normal&quot;, &quot;Bad&quot;],                    filled&#x3D;True, rounded&#x3D;True,                    special_characters&#x3D;True)graph &#x3D; graphviz.Source(TREE_Vis)graphpredictions3 &#x3D; model3.predict(playerTestDF_quant)conf_matrix3 &#x3D; confusion_matrix(playerTestLabel, predictions3)accuracy3 &#x3D; accuracy_score(playerTestLabel, predictions3)print(conf_matrix3)print(accuracy3)sns.heatmap(conf_matrix3, annot&#x3D;True,cmap&#x3D;&#39;Blues&#39;,xticklabels&#x3D;labels, yticklabels&#x3D;labels, cbar&#x3D;False)plt.title(&quot;Confusion Matrix&quot;,fontsize&#x3D;20)plt.xlabel(&quot;Actual&quot;, fontsize&#x3D;15)plt.ylabel(&quot;Predicted&quot;, fontsize&#x3D;15)plt.show()</code></pre>]]></content>
      
      
      <categories>
          
          <category> Decision Tree </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Code </tag>
            
            <tag> Decision Trees </tag>
            
            <tag> Supervised Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decision Tree Data Preparation</title>
      <link href="/2024/03/22/dt-data/"/>
      <url>/2024/03/22/dt-data/</url>
      
        <content type="html"><![CDATA[<h1 id="Data-Preparationg"><a href="#Data-Preparationg" class="headerlink" title="Data Preparationg"></a>Data Preparationg</h1><p>Select columns of <code>season</code>, <code>teamName</code>, <code>playerName</code>, <code>position</code>, <code>pass_accura</code>, <code>tackle_blocks</code>, <code>tackle_intercep</code>, and <code>fouls_committed</code> in the player dataset. Then, choose players whose position is the defender and playing season from 2020 to 2022. Dropping rows that are <code>pass_accura</code>, <code>tackle_blocks</code>, <code>tackle_intercep</code>, and <code>foul_committed</code> all are equal to 0. The main reason is these players are transferred by clubs at the beginning of the season or have never player in La Liga. After generating a new data set.<br><img src="/DT/data/1.jpg" alt="The Players Dataset"></p><p><img src="/DT/data/2.jpg" alt="The New Dataset"></p><p>Since the new dataset does not have a label column. Therefore, using k-mean clustering based on numeric variables of <code>pass_accura</code>, <code>tackle_bolcks</code>, <code>tackle_intercep</code> and <code>fouls_committed</code>. The k-value used here is 3 (Categories: Good performance, normal performance, and bad performance). After K-mean clustering, the label columns are merged into the new dataframe, as shown below.</p><p><img src="/DT/data/3.jpg" alt="The New Dataset with label(Performance) column"></p><h1 id="Train-Data-and-Test-Data-Splitting"><a href="#Train-Data-and-Test-Data-Splitting" class="headerlink" title="Train Data and Test Data Splitting"></a>Train Data and Test Data Splitting</h1><pre class="language-Python" data-language="Python"><code class="language-Python">playerTrainDF, playerTestDF &#x3D; train_test_split(player,test_size &#x3D; 0.3, random_state&#x3D;42)playerTrainDF.to_csv(&quot;DTTrainDF.csv&quot;)playerTestDF.to_csv(&quot;DTTestDF.csv&quot;)print(playerTrainDF)print(playerTestDF)</code></pre><p>Also using the train_split function as same in the <a href="http://hleof.github.io/2024/03/14/nb-data/#Train-Dataset-and-Test-Dataset-Splitting">Multinomial NB section</a>, and getting the train dataset and test dataset. Here the test is divided into 30% of the new dataframe. Also, use reandom_state&#x3D;42 to ensure that the train set and test set do not change once running code every time. Creating a disjoint split is essential for the prediction model. If there are overlapping samples in the training set and test dataset, the model will view the labels in the test set and remember them during training. It can lead to errors in the evaluation of the performance of the model. Just as a student knows the answers to an exam in advance, a teacher cannot determine whether this student has really learned content by the test score.<br>Using the code below to check whether train set and test set disjoint splitting or not.</p><pre class="language-Python" data-language="Python"><code class="language-Python">disjoint_check &#x3D; pd.merge(playerTrainDF, playerTestDF, how&#x3D;&#39;inner&#39;)# Check Train and Test sets are disjointif not disjoint_check.empty:    print(&quot;Train and Test set have same rows&quot;)else:    print(&quot;Train and Test set have not same rows&quot;)</code></pre><table><thead><tr><th><img src="/DT/data/4.jpg" alt="Train Dataframe"></th><th><img src="/DT/data/5.jpg" alt="Test Dataframe"></th></tr></thead><tbody><tr><td>The Train Dataset<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/DT/DTTrainDF.csv">Train Set</a></td><td>The Test Dataset<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/DT/DTTestDF.csv">Test Set</a></td></tr></tbody></table><p>After that, the train set and test will be fine-tuned using the following code in preparation for use with the decision tree algorithm.</p><pre class="language-Python" data-language="Python"><code class="language-Python">playerTestLabel &#x3D; playerTestDF[&#39;performance&#39;]playerTestDF &#x3D; playerTestDF.drop([&#39;performance&#39;],axis &#x3D; 1)playerTrainDF_nolabel &#x3D; playerTrainDF.drop([&#39;performance&#39;],axis &#x3D; 1)playerTrainLabel &#x3D; playerTrainDF[&#39;performance&#39;]dropcols &#x3D; [&#39;season&#39;,&#39;teamName&#39;,&#39;playerName&#39;,&#39;position&#39;]playerTrainDF_nolabel_quant &#x3D; playerTrainDF_nolabel.drop(dropcols,axis &#x3D; 1)playerTestDF_quant &#x3D; playerTestDF.drop(dropcols,axis&#x3D;1)</code></pre><hr><h1 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h1><p>Decision Tree Data Preparation Code <a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/DT/DT_pre_data.py">Python Code</a></p>]]></content>
      
      
      <categories>
          
          <category> Decision Tree </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Preparation </tag>
            
            <tag> Decision Trees </tag>
            
            <tag> Supervised Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decision Tree Overview</title>
      <link href="/2024/03/22/dt-intro/"/>
      <url>/2024/03/22/dt-intro/</url>
      
        <content type="html"><![CDATA[<h1 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h1><p>Decision tree is a supervised learning algorithm for classification and regression tasks. It is a hierarchical tree mechanism that combines root nodes, branches, internal nodes, and leaf nodes.</p><ul><li><p>Root Node: It contains the whole set of samples.</p></li><li><p>Internal Nodes: It is corresponding feature attributes test</p></li><li><p>Leaf Nodes: It is decision results.</p></li></ul><p>According to the figure shown below, this sample decision tree does not have any incoming branches from the root node, and then the branches from the root node start to provide information to the internal nodes (which are the decision nodes.). These two types of nodes both perform evaluations based on available functionality from a subset of the same class. The leaf nodes are these subsets and represent all possible decision outcomes.</p><p><img src="/DT/intro/1.png" alt="Structure of Decision Tree"></p><p>Hers is a simple example below, the CS department of a university to determine whether students can apply for the Capstone in their junior year. There are two indicators: GPA and whether students have taken advanced courses. From the figure blow, students with a GPA greater than 3.75 that can take Capstone course. If the students’ GPA are less than 3.75, it is necessary to determine whether the students had taken advance course. If they already did, they can. Otherwise, they cannot take the Capstone course. </p><p><img src="/DT/intro/2.jpg" alt="Decision Tree Example"></p><h2 id="Steps-for-Decision-Tree"><a href="#Steps-for-Decision-Tree" class="headerlink" title="Steps for Decision Tree"></a>Steps for Decision Tree</h2><ul><li><p><strong>Feature Selection:</strong> It determines which features do the judging. In the training dataset, each sample may have multiple attributes with different features playing a greater or lesser role. Thus, the function of feature selection is to pick the feature with a high correlation of classification results (The feature with strong classification ability). Some ways to select optimized feature such as Gini, Entropy, and Information Gain, and this part will be discussed later.</p></li><li><p><strong>Decision Tree Generation:</strong> Assuming that using information gain is the most important feature selection criterion. Once the features are selected, the information gain of all the features is computed from all nodes. Then, the feature with the largest information gain is selected as a node and leaf nodes are created based on the different values of that feature. Using this method and so on until there is small information gain or no features to choose from.</p></li><li><p><strong>Decision Tree Pruning:</strong> Its main purpose is to against overfitting, and it actively removes some branches to reduce the risk of overfitting.</p></li></ul><h2 id="Pros-and-Cons-of-Decision-Tree"><a href="#Pros-and-Cons-of-Decision-Tree" class="headerlink" title="Pros and Cons of Decision Tree"></a>Pros and Cons of Decision Tree</h2><p><code>Advantage:</code> It is easy to explain, and its Boolean logic and visualization help people to understand and use it. Also, the hierarchical structure of the decision tree makes it easier to spot essential features. It can handle discrete or continuous data types. It is also capable of handling data with missing values. </p><p><code>Disadvantage:</code> Complex decision trees can overfit and do not work well with new data(decision tree pruning mentioned above can be avoided). Also, it is constructed using a greedy search method and its tanning cost will be higher than other algorithms.</p><h1 id="Choosing-The-Best-Way-to-Divide"><a href="#Choosing-The-Best-Way-to-Divide" class="headerlink" title="Choosing The Best Way to Divide"></a>Choosing The Best Way to Divide</h1><p>The most important step in a decision tree is to choose an appropriate division way. Typically, as the division proceeds, the developers expect the branch nodes of the decision tree to contain samples that belong to the same category as much as possible to increase the purity of the nodes.  For example, to identify the quality of seeds, the final expectation is to group good seeds with good seeds and bad seeds with bad seeds as much as possible. Information Gain (including Entropy) and Gini Impurity are commonly as segmentation metrics in decision trees, and they are used to evaluate the contribution of each test condition to classification and the ability to improve node purity.</p><h2 id="Information-Gain"><a href="#Information-Gain" class="headerlink" title="Information Gain"></a>Information Gain</h2><p>Information gain is based on the change in entropy to determine the extent to which features contribute to the task of classification. The greater the information gain, the better classification results are obtained for feature division. Information entropy is the measure of uncertainty in datasets. Therefore, information gain is a measure of the effectiveness of features in reducing uncertainty and improving the purity of the datasets.</p><p>$$\text{Information Gain}(S,a) &#x3D; \text{Entropy(S)} - \sum_{j&#x3D;1}^{K}\frac{|S_j|}{|S|} \text{Entropy}(S_j)$$</p><p>$\text{a: Specific attributes or category label}$</p><p>$\text{S: Infomration Entropy of the dataset}$</p><p>$\sum_{j&#x3D;1}^{K} \frac{|S_j|}{|S|} \text{Entropy}(S_j): \text{The conditional entropy of each feature}$</p><blockquote><p><code>Information gain</code> is a measure quantifies how much a given <code>tree node split</code> unmixes the labels at a node. Mathematically it is measure of the difference between impurity values before splitting the data at a node and the weighted average of the impurity after the split<br>source from: <a href="https://gatesboltonanalytics.com/?page_id=278">https://gatesboltonanalytics.com/?page_id=278</a></p></blockquote><h3 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h3><p>Entropy is a metric used to measure the uncertainty of a dataset, the higher the entropy value, the higher the uncertainty of the dataset. This situation leads to a subset of a dataset containing a greater mix of different categories. Thus, during the division process, make sure that the lower the entropy value, which cases higher the purity of the divided subset.</p><p>$$\text{Entropy} &#x3D; \sum_{i&#x3D;1}^{C} - p_ilog_2(p_i) $$</p><p>$\text{S: Infomration entropy of dataset}$</p><p>$\text{C: categories in the set}$</p><p>$p_i:\text{Proportion of data points in category i to the total number of points in the dataset}$</p><h3 id="Gini"><a href="#Gini" class="headerlink" title="Gini"></a>Gini</h3><p>Gini impurity is the probability of misclassification after classifying the data based on some feature. The lower the value of Gini, the more samples of the same category are included in the subset and the better affection of the division. If the dataset cannot be further divided, then its impurity is zero.</p><p>$$\text{Gini} &#x3D; 1 - \sum_{i&#x3D;1}^{C}{(p_i)^2}$$</p><p>During the processing of building a decision tree, using Gini as a picking feature method is a good way once encounter a dataset that contains a larger number of categorical categories, or to evaluate the nodes of the decision tree.</p><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p><img src="/DT/intro/3.jpg" alt="Dataset Sample"></p><h3 id="Entropy-and-Information-Gain"><a href="#Entropy-and-Information-Gain" class="headerlink" title="Entropy and Information Gain"></a>Entropy and Information Gain</h3><ul><li>Calculate the entropy of origial dataset<br>$$P(\text{Play Soccer &#x3D; Yes}) &#x3D; \frac{3}{5} &#x3D; 0.6 \text{, and }  P(\text{Play Soccer &#x3D; No}) &#x3D; \frac{2}{5} &#x3D; 0.4$$</li></ul><p>$$ Entropy(S) &#x3D; \sum_{i&#x3D;1}^{C} - p_ilog_2(p_i) &#x3D; -0.6log_2{0.6} - 0.4log_2{0.4} \approx 0.9710$$</p><ul><li>Calcualte conditional entropy of Weather</li></ul><p>$$P(\text{P&#x3D; Yes | Weather &#x3D; Sunny}) &#x3D; \frac{1}{2} &#x3D; 0.5 \text{, and }  P(\text{P&#x3D;No | Weather &#x3D; Sunny}) &#x3D; \frac{1}{2} &#x3D; 0.5$$</p><p>$$P(\text{P&#x3D;Yes | Weather &#x3D; Cloud}) &#x3D; \frac{2}{3} \text{, and }  P(\text{P &#x3D; No | Weather &#x3D; Cloud}) &#x3D; \frac{1}{3}$$</p><p>$$Entropy(S_\text{Sunny}) &#x3D; \sum_{i&#x3D;1}^{2} - p_ilog_2(p_i) &#x3D; -0.5log_2{0.5} - 0.5log_2{0.5} &#x3D; 1$$</p><p>$$Entropy(S_\text{Cloud}) &#x3D; \sum_{i&#x3D;1}^{2} - p_ilog_2(p_i) &#x3D; -\frac{2}{3}log_2{\frac{2}{3}} - \frac{1}{3}log_2{\frac{1}{3}} &#x3D; 0.9183$$</p><p>$$H(S|Weather) &#x3D; \sum_{j&#x3D;1}^{2}\frac{|S_\text{Weather}|}{|S|} \text{Entropy}(S_\text{Weather}) &#x3D; \frac{2}{5} * 1+ \frac{3}{5} * 0.9183 &#x3D; 0.9510$$</p><ul><li>Calcuate the conditional entropy of Temperature</li></ul><p>$$P(\text{P&#x3D; Yes | Temp &#x3D; Hot}) &#x3D; \frac{1}{2} \text{, and }  P(\text{P&#x3D;No | Temp &#x3D; Hot}) &#x3D; \frac{1}{2}$$</p><p>$$P(\text{P&#x3D;Yes | Temp &#x3D; Cool}) &#x3D; \frac{2}{2} \text{, and }  P(\text{P &#x3D; No | Temp &#x3D; Cool}) &#x3D; \frac{0}{2}$$</p><p>$$P(\text{P&#x3D;Yes | Temp &#x3D; Cold}) &#x3D; \frac{0}{1} \text{, and }  P(\text{P &#x3D; No | Temp &#x3D; Cold}) &#x3D; \frac{1}{1}$$</p><p>$$Entropy(S_\text{Hot}) &#x3D; \sum_{i&#x3D;1}^{2} - p_ilog_2(p_i) &#x3D; -0.5log_2{0.5} - 0.5log_2{0.5} &#x3D; 1$$</p><p>$$Entropy(S_\text{Cool}) &#x3D; \sum_{i&#x3D;1}^{2} - p_ilog_2(p_i) &#x3D; -\frac{2}{2}log_2{\frac{2}{2}} &#x3D; 0$$</p><p>$$Entropy(S_\text{Cold}) &#x3D; \sum_{i&#x3D;1}^{2} - p_ilog_2(p_i) &#x3D; -\frac{1}{1}log_2{\frac{1}{1}} &#x3D; 0$$</p><p>$$H(S|Temp) &#x3D; \sum_{j&#x3D;1}^{2}\frac{|S_\text{Temp}|}{|S|} \text{Entropy}(S_\text{Temp}) &#x3D; \frac{2}{5} * 1+ \frac{2}{5} * 0 + \frac{1}{5}*0 &#x3D; 0.4$$</p><ul><li>Calcuate the information gain for each feature</li></ul><p>$$\text{Information Gain}(S,Weather) &#x3D; \text{Entropy(S)} - \sum_{j&#x3D;1}^{K}\frac{|S_j|}{|S|} \text{Entropy}(S_j) &#x3D; 0.9710 - 0.9510 &#x3D;  0.02$$</p><p>$$\text{Information Gain}(S,Temp) &#x3D; \text{Entropy(S)} - \sum_{j&#x3D;1}^{K}\frac{|S_j|}{|S|} \text{Entropy}(S_j) &#x3D; 0.9710 - 0.4 &#x3D;  0.571$$</p><p>Therefore, choosing feature of Temperature as the first split point, because $\text{Information Gain}(S,Temp) &gt; \text{Information Gain}(S,Weather)$</p><p><img src="/DT/intro/4.jpg" alt="Demo for Splitting Process"></p><h1 id="Create-an-infinite-number-of-trees"><a href="#Create-an-infinite-number-of-trees" class="headerlink" title="Create an infinite number of trees"></a>Create an infinite number of trees</h1><p>In each split of a node, the decision tree can select the best-split point from multiple features. Also, the continuity features are able to choose different division points, so this allows the decision tree to create many different topologies. Second, for large-scale datasets with many features, a decision tree may create deeper trees to better fit the data. It can be able to classify or regress the data better.</p><h1 id="Plan-For-the-Project"><a href="#Plan-For-the-Project" class="headerlink" title="Plan For the Project"></a>Plan For the Project</h1><p>Analyzing the defenders key capabilities of LaLiga, including tackle blocks, tackle interception, and fouls committed. Using decision tree algorithms to predict whether the summed ability of the defensive players meets realistic expectations. Whether their overall ability is a key factor in a club’s ranking.</p><hr><h1 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h1><p>image1: <a href="https://images.app.goo.gl/zW6zUWat4zN1hwTCA">https://images.app.goo.gl/zW6zUWat4zN1hwTCA</a></p><p>reference1 : <a href="https://www.ibm.com/cn-zh/topics/decision-trees">https://www.ibm.com/cn-zh/topics/decision-trees</a></p><p>reference2: <a href="https://zyzypeter.github.io/2017/07/25/machine-learning-ch6-decision-tree/#%E9%80%89%E6%8B%A9%E6%9C%80%E4%BC%98%E5%88%92%E5%88%86%E6%96%B9%E5%BC%8F">https://zyzypeter.github.io/2017/07/25/machine-learning-ch6-decision-tree/#%E9%80%89%E6%8B%A9%E6%9C%80%E4%BC%98%E5%88%92%E5%88%86%E6%96%B9%E5%BC%8F</a></p>]]></content>
      
      
      <categories>
          
          <category> Decision Tree </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Overview </tag>
            
            <tag> Decision Trees </tag>
            
            <tag> Supervised Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Naive Bayes Conclusion</title>
      <link href="/2024/03/14/nb-conclu/"/>
      <url>/2024/03/14/nb-conclu/</url>
      
        <content type="html"><![CDATA[<p><img src="/NB/result/5.jpg" alt="The Best Forward Players in LaLiga"></p><p>The overall performance of a forward player in La Liga can be predicted by a series of key statistics, such as playing time, shooting conversion rate, and success rate of passing. While these data are important to assessing a forward’s ability, the combined ability of a forward is just one of many key factors that can affect a club’s ranking. Thus, it can be said that achieving a win for each game and achieving the desired ranking also requires a combination of other factors such as fewer injures, the best attacking strategy, or emotional influence.  </p>]]></content>
      
      
      <categories>
          
          <category> Naive Bayes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Conclusion </tag>
            
            <tag> Unsupervised Machine Learning </tag>
            
            <tag> Naive Bayes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Naive Bayes Results</title>
      <link href="/2024/03/14/nb-result/"/>
      <url>/2024/03/14/nb-result/</url>
      
        <content type="html"><![CDATA[<h1 id="Accuracy-of-Model’s-Prediction"><a href="#Accuracy-of-Model’s-Prediction" class="headerlink" title="Accuracy of Model’s Prediction"></a>Accuracy of Model’s Prediction</h1><p>The prediction results are shown through the following the confusion matrix and line plot. The confusion matrix can evaluate the prediction performance of this multinomial Naive Bayes model. First of all, it can calculate the accuracy of the model through this matrix. According to the formula of accuracy:<br>$$ \frac{\text{Number of corretly classified Samples}}{\text{Total Number of Sampels}} &#x3D; \frac{10+5+27}{10+0+5+1+5+4+3+1+27} &#x3D; \frac{42}{56} \approx 0.75$$</p><p>The accuracy of the model’s prediction is 75%. This result is the same as the result of accuracy_score in the Sklearn library.</p><pre class="language-Python" data-language="Python"><code class="language-Python">from sklearn.metrics import accuracy_scoreaccuracy &#x3D; accuracy_score(playerTestLabel, prediction)print(accuracy)&gt;&gt;&gt; 0.75</code></pre><p><img src="/NB/result/1.png" alt="Confusion Matrix"></p><p><img src="/NB/result/3.png" alt="Test Label vs. Prediction Label"></p><p>The line plot also reveals that roughly 35% of the folds in the plot are not overlapped. The 75% correct accuracy of prediction may indicate that the predictive ability of this model is at intermediate to upper intermediate level. Thus, the predicted results that can be convincing.</p><p>Not only that, getting the precision rate for each category by using the confusion matrix:</p><p>$$\text{‘Good’ Precision} &#x3D; \frac{10}{10+1+3} &#x3D; \frac{10}{14} \approx 0.71$$</p><p>$$\text{‘Normal’ Precision} &#x3D; \frac{5}{0+5+1} &#x3D; \frac{5}{6} \approx 0.83$$</p><p>$$\text{‘Bad’ Precision} &#x3D; \frac{24}{5+4+27} &#x3D; \frac{24}{36} \approx 0.75$$</p><p>According to the these formulas can be seen that the category of <code>Nomral</code> has highest precision rate, which is about 83%. The next highest precision rate for the category of <code>Bad</code> is about 75%. The category of <code>Good</code> has a precision rate of about 71%. Therefore, predicting the category as <code>Normal</code> is the most accurate, and predicting the category as <code>Good</code> is less effective than the other two categories. </p><p>From the following model prediction probability, if there is a situation like Good &#x3D; 0.33, Normal &#x3D; 0.33, Bad &#x3D; 0.34 or Good &#x3D; 0.47, Normal &#x3D; 0.48, Bad &#x3D; 0.05.  It indicates that the model has high uncertainty when classifying samples. In this case, the model fails to clearly determine which categories the sample belongs to and distribute the probabilities across multiple categories. However, this model does not appear in this situation which shows that the classification accuracy is still acceptable.</p><p><img src="/NB/result/2.png" alt="Model Prediction Prability"></p><p>The following is a part of the predicted dataset. The model’s predictions are good. For example, Real Madrid attacker Hazard. He is a player that Real Madrid bought from Chelsea at a high price (He had a super good performance at Chelsea.), but he had a very poor couple of seasons at Real Madrid with a lot of injuries, which led to his direct retirement from Real Madrid. Thus, his performance prediction turned out to be Bad. Celta Vigo’s Aspas is a decisive figure in the club and he has been very good in recent seasons, so his performance prediction turned out to be good. Therefore, these predictions are in line with reality. However, predicting how good a forward player is in Laliga is not a direct determinant of the immediate factors in a club’s ranking. Just like Hazard has poor performance, but Real Madrid has a high ranking. Aspas is playing well but his team is ranked low.</p><p><img src="/NB/result/4.jpg" alt="Attacker Preformance Whether Affect Ranking of A Club"></p>]]></content>
      
      
      <categories>
          
          <category> Naive Bayes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Results </tag>
            
            <tag> Supervised Machine Learning </tag>
            
            <tag> Naive Bayes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Naive Bayes Code</title>
      <link href="/2024/03/14/nb-code/"/>
      <url>/2024/03/14/nb-code/</url>
      
        <content type="html"><![CDATA[<h1 id="Multinomial-Naive-Bayes-Algorithm-Python-Code"><a href="#Multinomial-Naive-Bayes-Algorithm-Python-Code" class="headerlink" title="Multinomial Naive Bayes Algorithm Python Code"></a>Multinomial Naive Bayes Algorithm Python Code</h1><p>Data Perparation Code <a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/NB/NB_pre_data.py">Python Code</a></p><p>Multinomial Naive Bayes Algorithm <a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/NB/Multinomial%20NB%20Model.py">Python Code</a></p><pre class="language-Python" data-language="Python"><code class="language-Python">import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom sklearn.model_selection import train_test_splitfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.metrics import confusion_matrixplayer &#x3D; pd.read_csv(&quot;label_clean_DF.csv&quot;)playerTrainDF, playerTestDF &#x3D; train_test_split(player,test_size &#x3D; 0.3)playerTestLabel &#x3D; playerTestDF[&#39;label&#39;]playerTestDF &#x3D; playerTestDF.drop([&quot;label&quot;],axis &#x3D; 1)playerTrainDF_nolabel &#x3D; playerTrainDF.drop([&quot;label&quot;],axis &#x3D; 1)playerTrainLabel &#x3D; playerTrainDF[&#39;label&#39;]dropcols &#x3D; [&#39;season&#39;,&#39;teamName&#39;,&#39;playerName&#39;,&#39;position&#39;]playerTrainDF_nolabel_quant &#x3D; playerTrainDF_nolabel.drop(dropcols,axis &#x3D; 1)playerTestDF_quant &#x3D; playerTestDF.drop(dropcols,axis&#x3D;1)model &#x3D; MultinomialNB()model.fit(playerTrainDF_nolabel_quant,playerTrainLabel)prediction &#x3D; model.predict(playerTestDF_quant)cnf_matrix &#x3D; confusion_matrix(playerTestLabel, prediction)print(&quot;\nThe confusion matrix is:&quot;)print(cnf_matrix)print(np.round(model.predict_proba(playerTestDF_quant),2))labels &#x3D; [&#39;Good&#39;,&#39;Normal&#39;,&#39;Bad&#39;]sns.heatmap(cnf_matrix, annot&#x3D;True,cmap&#x3D;&#39;Blues&#39;,xticklabels&#x3D;labels, yticklabels&#x3D;labels, cbar&#x3D;False)plt.title(&quot;Confusion Matrix&quot;,fontsize&#x3D;20)plt.xlabel(&quot;Actual&quot;, fontsize&#x3D;15)plt.ylabel(&quot;Predicted&quot;, fontsize&#x3D;15)prob_matrix &#x3D; np.round(model.predict_proba(playerTestDF_quant),2)plt.figure(figsize&#x3D;(15, 15))sns.heatmap(prob_matrix, annot&#x3D;True,cmap&#x3D;&#39;Blues&#39;,xticklabels&#x3D;labels, cbar&#x3D;False)plt.title(&quot;Model Prediction Probability&quot;,fontsize&#x3D;20)plt.xlabel(&quot;Labels&quot;, fontsize&#x3D;15)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Naive Bayes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Code </tag>
            
            <tag> Supervised Machine Learning </tag>
            
            <tag> Naive Bayes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Naive Bayes Data Preparation</title>
      <link href="/2024/03/14/nb-data/"/>
      <url>/2024/03/14/nb-data/</url>
      
        <content type="html"><![CDATA[<h1 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h1><p>Selecting columns of <code>season</code>, <code>teamName</code>, <code>playerName</code>, <code>position</code>, <code>game_appear</code>, <code>game_lineups</code>, <code>game_minutes</code>. <code>key_pass</code>, <code>pass accuracy</code>, and <code>fouls drawn</code> in the <a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/NB/clean_laliga_playerDF.csv">player dataset</a>. Calculating shot success rate by <code>goals_total/shots_total</code>, calculating duel success rate by <code>duel_won/duel_total</code>, and calculating dribble success rate by <code>dribble_success/dribble_attempt.</code> Then, pick players whose position is the attacker and season from 2020 to 2022. There are some NaN values due to some players being transferred by clubs at the beginning of the season, so dropping these NaN values. After that, generate a new data frame. </p><p><img src="/NB/data/1.jpg" alt="The Player Dataset"></p><p><img src="/NB/data/2.jpg" alt="The New Datafram"></p><p>Since this new dataframe does not have a label column, it does not work for using multinomial Naïve Bayes, so it is necessary to add a label column. Data in this dataset is used to predict whether the performance of an attacker is good or not. Thus, the label is divided into three categories: good, normal, and bad. Due to <code>game_appear</code>, <code>game_lineups</code>, <code>game_minutes</code>, <code>key_pass</code>, <code>pass_accuracy</code>, <code>foul_drawn</code>, <code>shot_success_rate</code>, <code>duel_success_rate</code>, <code>and _dirbble_success_rate</code> are numeric variables, so it needs to create labels for all of them (still three categories: good, normal, bad). Finally, these variable labels are aggregated to generate a final label for each row.</p><p>Plot the boxplot of each variable to determine how to divide categories by numeric value, then add a label column for each variable to the dataframe.</p><p><img src="/NB/data/3.png" alt="The boxplot for each numeric variable"></p><p><img src="/NB/data/4.jpg" alt="Labels For Each Numeric Variabel"></p><p>Using the Python code below, count which categories appear the most in each row and serve as the final label. And final dataframe below is </p><pre class="language-python" data-language="python"><code class="language-python">cols <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'game_appear_labels'</span><span class="token punctuation">,</span><span class="token string">'game_lineups_labels'</span><span class="token punctuation">,</span><span class="token string">'game_minutes_labels'</span><span class="token punctuation">,</span>        <span class="token string">'shot_success_rate_labels'</span><span class="token punctuation">,</span><span class="token string">'duel_success_rate_labels'</span><span class="token punctuation">,</span><span class="token string">'dribble_success_rate_labels'</span><span class="token punctuation">,</span>        <span class="token string">'key_pass_labels'</span><span class="token punctuation">,</span><span class="token string">'pass_accura_labels'</span><span class="token punctuation">,</span><span class="token string">'fouls_drawn_labels'</span><span class="token punctuation">]</span>player<span class="token punctuation">[</span><span class="token string">'label'</span><span class="token punctuation">]</span> <span class="token operator">=</span> player<span class="token punctuation">[</span><span class="token punctuation">[</span>col <span class="token keyword">for</span> col <span class="token keyword">in</span> cols<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>mode<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></code></pre><p><img src="/NB/data/5.jpg" alt="Final DataFrame"></p><h1 id="Train-Dataset-and-Test-Dataset-Splitting"><a href="#Train-Dataset-and-Test-Dataset-Splitting" class="headerlink" title="Train Dataset and Test Dataset Splitting"></a>Train Dataset and Test Dataset Splitting</h1><pre class="language-Python" data-language="Python"><code class="language-Python">import pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.metrics import confusion_matrixplayer &#x3D; pd.read_csv(&quot;label_clean_DF.csv&quot;)playerTrainDF, playerTestDF &#x3D; train_test_split(player,test_size &#x3D; 0.3)playerTestLabel &#x3D; playerTestDF[&#39;label&#39;]playerTestDF &#x3D; playerTestDF.drop([&quot;label&quot;],axis &#x3D; 1)playerTrainDF_nolabel &#x3D; playerTrainDF.drop([&quot;label&quot;],axis &#x3D; 1)playerTrainLabel &#x3D; playerTrainDF[&#39;label&#39;]dropcols &#x3D; [&#39;season&#39;,&#39;teamName&#39;,&#39;playerName&#39;,&#39;position&#39;]playerTrainDF_nolabel_quant &#x3D; playerTrainDF_nolabel.drop(dropcols,axis &#x3D; 1)playerTestDF_quant &#x3D; playerTestDF.drop(dropcols,axis&#x3D;1)</code></pre><p>Using the train_test_split function, and getting the train set and test set (shown below).   Here the test dataset is divided into 30% of the total dataframe.   This allows us to evaluate the generalization ability of the multinomial NB model divided by the final dataframe 70% as the training dataset, and the final dataset 30% as the test dataset, Creating a disjoint split is essential for the prediction model.   If there are overlapping samples in the training set and test dataset, the model will view the labels in the test set and remember them during training.   It can lead to errors in the evaluation of the performance of the model.   Just as a student knows the answers to an exam in advance, a teacher cannot determine whether this student has really learned content by the test score.</p><table><thead><tr><th><img src="/NB/code/1.jpg" alt="Train Dataframe"></th><th><img src="/NB/code/2.jpg" alt="Test Dataframe"></th></tr></thead><tbody><tr><td>The Train Dataset<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/NB/playerTrainDataset.csv">Train Set</a></td><td>The Test Dataset<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/NB/playerTestDataset.csv">Test Set</a></td></tr></tbody></table><hr><h1 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h1><p>The Final Dataset<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/NB/label_clean_DF.csv">Final Dataframe</a></p><p>The Data Preparation Code<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/NB/NB_pre_data.py">Python Code</a></p>]]></content>
      
      
      <categories>
          
          <category> Naive Bayes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Preparation </tag>
            
            <tag> Supervised Machine Learning </tag>
            
            <tag> Naive Bayes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction of Naive Bayes (NB)</title>
      <link href="/2024/03/14/nb-intro/"/>
      <url>/2024/03/14/nb-intro/</url>
      
        <content type="html"><![CDATA[<h1 id="Bayes-Theorem"><a href="#Bayes-Theorem" class="headerlink" title="Bayes Theorem"></a>Bayes Theorem</h1><p>It is a theorem in probability theory that describes the probability that something will happen given some known conditions. For example, the duration of smoking is relatively high with lung cancer. Thus, using  the formula of the Bayes theorem to accurately calculate how likely someone is to get lung cancer depending on their duration of smoking.</p><p>$$P(A | B) &#x3D; \frac{P(A)P(B|A)}{P(B)}$$</p><p>$P(A | B)$: It is the <em><strong>posterior probability</strong></em> of A given the predictor B</p><p>$P(B | A)$: It is the <em><strong>likelihood</strong></em>, that probability of B given the A</p><p>$P(A)$: It is <em><strong>A Prior Probability</strong></em> that is probability of the A</p><p>$P(B)$: It is <em><strong>B prior Probability</strong></em> that is probability of the B</p><blockquote><p>Here is an example that using the Bayes theorem<br>Doctors know that hyperthyroidism(H) causes a thick neck(T) is  60%, So $P(T|H) &#x3D; 0.6$<br>Prior probability of any patient having hyperthyroidism is $\frac{1}{300}$, So $P(H) &#x3D; \frac{1}{300}$<br>Prior probability of any patient having thick neck is $\frac{1}{60}$, So $P(T) &#x3D; \frac{1}{60}$<br>The probability of a patient having hyperthyroidism if he&#x2F;she has a thick neck.<br>$$P(H|T) &#x3D; \frac{P(H)P(T|H)}{P(T)} &#x3D; \frac{\frac{1}{300}*0.6}{\frac{1}{60}} &#x3D; 0.12$$<br>Therefore, if a patient has a thick neck, he&#x2F;she has the probability of hyperthyroidism is 12%.</p></blockquote><h2 id="Bayesian-Classifiers"><a href="#Bayesian-Classifiers" class="headerlink" title="Bayesian Classifiers"></a>Bayesian Classifiers</h2><p>Bayes classifier is a class of classification algorithms based on Bayes theorem; Naïve Bayes classifier is one of them. It uses the Bayes theorem and calculates the posterior probability of each category based on the conditional probability of the input features, then selects the features with the highest posterior probability as the prediction results.</p><h2 id="Naive-Bayes-Algorithm"><a href="#Naive-Bayes-Algorithm" class="headerlink" title="Naive Bayes Algorithm"></a>Naive Bayes Algorithm</h2><p>The Naive Bayes algorithm is a machine learning classification algorithm based on Bayes theorem, and it is also a supervised learning algorithm. It will classify datasets by using Bayes’ theorem and the assumption of independence between features. In the real world, it is used for text classification, spam filtering, and sentiment analysis, etc.</p><p>There are serval variations of the Naive Bayes algorithm, such as Gaussian Naive Bayes, multinomial Naive Bayes, and Bernoulli Naive Bayes.  These variations may apply to different types of features such as continuous, and discrete data types, and also apply to different distributions such as multinomial, gaussian, etc.  Its advantage is that the simply implements the algorithm and is computationally efficient.  It works well for high-dimensional data and large-scale datasets. But it assumes that the features are independent of each other, which is inconsistent in some cases in reality.  </p><h1 id="Multinomial-Naive-Bayes"><a href="#Multinomial-Naive-Bayes" class="headerlink" title="Multinomial Naive Bayes"></a>Multinomial Naive Bayes</h1><p>Multinomial Naive Bayes is a kind of classification algorithm based on the Bayes theorem, and it is often used for text classification. It assumes that the probability distribution of the text sample is a multinomial distribution (Each feature is a discrete count value). These features could be words or phrases, and the value of each feature is the number of times the word or phrase appears in the text. In text classification, the text data needs to be converted into feature vectors at first, which can be represented by TF-IDF. Then, use a trained dataset to train a multinomial model to estimate the conditions probability distribution for each category. Finally, based on Bayes theorem, calculating the posterior probability of each category, and then picking a category with the greatest posterior probability as the final classification result.</p><p>The process of text classification using Multinomial NB can be divided into two main steps (The figure shown below).</p><p><strong>Training Phase</strong></p><ol><li><p>Count the number of documents for each category from the training dataset and calculate the prior probability of the category. </p></li><li><p>For each category, count the frequency of each feature under the category, and calculate the conditional probability of the feature under the given category.</p></li></ol><p><strong>Prediction Phase</strong></p><ol><li><p>For each category, calculate the posterior probability that the new documents belong to that category. </p></li><li><p>Select the category with the highest posterior probability as the prediction results.<br><img src="/NB/overviews/1.jpg" alt="The Flowchart for Applying Multinomial NB"></p></li></ol><h2 id="Smoothing"><a href="#Smoothing" class="headerlink" title="Smoothing"></a>Smoothing</h2><p>Some features never appear in a category, which leads to a value of zero when calculating the condition probabilities. This situation may cause the model to be less predictive of never-before-seen features when predicted and may affect the model’s ability to generalize to new data. Thus, using the smoothing to deal with this issue. Common smoothing techniques such as Laplace smoothing and Lidstone smoothing. The main idea is to ensure the probability estimates for all features under each category are non-zero by adjusting the numerator and denominator of conditional probabilities. Smoothing is adding a small constant to the probability for each feature or adjusting the denominator to avoid it being zero. Therefore, using smoothing can prevent zero probability appear due to unseen features and improve the performance of the Multinomial NB model.</p><p>$$\text{Probability Estimation(Smoothing)}$$</p><p>$$\text{C:number of Classes, p: prior probability, m &#x3D; parameter}$$</p><p>$$\text{Laplace:} P(A_i|C) &#x3D; \frac{N_{iC}+1}{N_C + C}$$</p><p>$$\text{m-estimate:} P(A_i|C) &#x3D; \frac{N_{iC}+mp}{N_C + m}$$</p><h1 id="Bernoulli-Naive-Bayes"><a href="#Bernoulli-Naive-Bayes" class="headerlink" title="Bernoulli Naive Bayes"></a>Bernoulli Naive Bayes</h1><p>Bernoulli Naive Bayes is another variant of the Naive Bayes classifier and is mainly used to process binary data.   It is also commonly used for text classification. It is different from multinomial Naive Bayes in that it mainly applies to binary features (the eigenvalues are usually 0 or 1). Therefore, the Bernoulli distribution is called a binary distribution, which means there are two possibilities for each event(Happen or not happen).  It also needs to use smoothing techniques to handle features that do not appear in the training dataset to avoid the occurrence of 0 probabilities.</p><h1 id="Plan-For-the-Project"><a href="#Plan-For-the-Project" class="headerlink" title="Plan For the Project"></a>Plan For the Project</h1><p>Analyzing the forwards(attackers) key capabilities of LaLiga, including passing accuracy, number of fouls, goal conversion rate, playing time, number of lines up, number of plays, number of key passes, and dribble success rate. And, using multinomial Naive Bayes to predict whether the summed ability of the forward players meets realistic expectations. Whether their overall ability is a key factor in a club’s ranking.</p>]]></content>
      
      
      <categories>
          
          <category> Naive Bayes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Overview </tag>
            
            <tag> Supervised Machine Learning </tag>
            
            <tag> Naive Bayes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Association Rule Mining Conclusion</title>
      <link href="/2024/02/20/arm-conclu/"/>
      <url>/2024/02/20/arm-conclu/</url>
      
        <content type="html"><![CDATA[<p><img src="/ARM/result/22.jpg" alt="La Liga Top Goalkeepers"></p><p>There exists a strong relationship between the goalkeeper’s saving ability and the team’s performance and ranking after exploring. In general, goalkeepers who perform well tend to have a high save ability, which helps to boost the ranking and performance of the club. On the contrary, goalkeepers with average or poor save ability can cause goals to be conceded, which affects the results of the clubs and their final ranking. It is worth noting that goalkeepers with poor saves can fall into a vicious cycle:</p><p>$$ \text{Poor Saving Ability} \to \text{Poor Performance on The Field} \to \text{Less Times To Play} \to \text{Poorer Saving Ability} $$</p><p>It not only can affect an individual player’s confidence, it can also have a negative impact on the overall team ranking. Therefore, the ability of a goalkeeper is crucial to the overall performance of the club. </p><hr><h1 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h1><p>image: <a href="https://images.app.goo.gl/VrQfqiiUqKYZEc647">https://images.app.goo.gl/VrQfqiiUqKYZEc647</a></p>]]></content>
      
      
      <categories>
          
          <category> Association Rule Mining </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Association Rule Mining </tag>
            
            <tag> Conclusion </tag>
            
            <tag> Unsupervised Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Association Rule Mining Results</title>
      <link href="/2024/02/20/arm-result/"/>
      <url>/2024/02/20/arm-result/</url>
      
        <content type="html"><![CDATA[<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>During exploring association rule mining, the thresholds hold finally determined after repeated testing and comparison results are follows:</p><p>$$ Support &#x3D; 0.01 $$</p><p>$$ Confidence &#x3D; 0.5 $$</p><p>$$ Min Length &#x3D; 2 $$</p><pre class="language-R" data-language="R"><code class="language-R">rules &lt;- arules::apriori(arm, parameter &#x3D; list(supp&#x3D;0.01, conf&#x3D;0.5, minlen&#x3D;2))</code></pre><h2 id="Support"><a href="#Support" class="headerlink" title="Support"></a>Support</h2><p><img src="/ARM/result/1.jpg" alt="Top 15 Rules For Support"><br><img src="/ARM/result/1-1.jpg" alt="Network Graph For Support"></p><p>Finding some interesting things through analyzing the top 15 rules for support and its visualization of results.</p><p>For $\text{Normal_Saved_Skill} \to \text{Normal_Preformance} (Support \approx 0.57 \And  Confidence \approx 0.95)$. It represents that a goalkeeper’s performance in saves is closely related to his overall performance on the field. Normally, a goalkeeper with average save ability can lead to conceding goals, which can affect the club’s results and ranking.</p><p>For $\text{Good_Preformance} \to \text{Good_Saved_Skill} (Support \approx 0.27 \And  Confidence \approx 0.9)$.It tells that a goalkeeper who performs well is likely to have excellent save ability, which may help to boost the team’s ranking.</p><p>For $\text{(Mid_Ranking, Normal_Saved_Skill)} \to \text{Normal_Preformance} (Support \approx 0.27 \And  Confidence \approx 1)$. It indicates that goalkeepers on teams in the middle table are likely to be average in their ability to make saves, so it reflects these goalkeepers’ average performance on the pitch.</p><h2 id="Confidence"><a href="#Confidence" class="headerlink" title="Confidence"></a>Confidence</h2><p><img src="/ARM/result/2.jpg" alt="Top 15 Rules For Confidence"><br><img src="/ARM/result/2-2.jpg" alt="Netwrok Graph For Confidence"></p><p>Getting some interesting results based on the top 15 rules for confidence and its node graph.</p><p>According to $\text{(Bad_Performance, Bad_Saved_Skill)} \to \text{LessTime_Games} rule$. It shows that a goalkeeper’s saving ability is bad, his performance on the pitch would also be affected, so the probability of getting a chance to play becomes small.  This situation can cause his performance to deteriorate further, and step into a vicious circle. Not only will undermine the goalkeeper’s own confidence, but it may also have a negative impact on the team’s overall results. </p><p>According to $\text{Bad_Saved_Skill} \to \text{Mid_Ranking} $ rule. It reflects on a common situation in reality. When a goalkeeper has a relatively low save ability and his team is in the middle of the table, this usually means that the goalkeeper is not the starting goalkeeper, and he may be the second or third goalkeeper of the team. As his save ability was not sufficient for the role of starting goalkeeper, he played relatively little in the game. However, the goalkeeper with poor save ability may be forced to replace the starting goalkeeper once the starting GK is injured and unable to play. Because clubs’ backup GK may not be able to provide the same level of performance as their starting GK, the teams’ results may be affected.</p><h2 id="Lift"><a href="#Lift" class="headerlink" title="Lift"></a>Lift</h2><p><img src="/ARM/result/3.jpg" alt="Top 15 Rules For Lift"><br><img src="/ARM/result/3-1.jpg" alt="Network Graph For Lift"></p><p>According to $\text{(Good_Saved_Skill, High_Ranking, Normal_Saved_Skill)} \to \text{NormalTime_Games}$ rule. It indicates that a goalkeeper is good on the field, and his team’s ranks also high. However, his chances of getting playing time are also average due to his average save ability. Therefore, this rule is quite realistic. For higher-ranked teams, a goalkeeper’s ability to save is crucial. As a typical example, Barcelona’s goalkeeper Peña has had some opportunities to play after Ter Stegen’s injury. He has had serval highlight performances at the beginning, but the team often loses the ball due to his average saving ability. </p>]]></content>
      
      
      <categories>
          
          <category> Association Rule Mining </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unsupervised Machine Learning </tag>
            
            <tag> Association Rule </tag>
            
            <tag> Results </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Association Rule Mining Code</title>
      <link href="/2024/02/20/arm-code/"/>
      <url>/2024/02/20/arm-code/</url>
      
        <content type="html"><![CDATA[<h1 id="ARM-Code"><a href="#ARM-Code" class="headerlink" title="ARM Code"></a>ARM Code</h1><p><em><strong>Data Paparation Code<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/ARM/ARM_Data_Prep.R">R Code</a></strong></em><br><em><strong>Transcation Data<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/ARM/arm.csv">CSV File</a></strong></em><br><em><strong>ARM Code<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/ARM/ARM.R">R Code</a></strong></em></p><pre class="language-R" data-language="R"><code class="language-R">install.packages(&quot;arulesViz&quot;)install.packages(&quot;arules&quot;)library(arulesViz)library(tidyverse)library(dplyr)library(arules)# Read CSV file data as Transaction Dataarm &lt;- read.transactions(&quot;arm.csv&quot;,                          rm.duplicates &#x3D; FALSE,                          format&#x3D;&quot;basket&quot;,                         sep &#x3D; &quot;,&quot;,                         cols&#x3D;NULL,                         header &#x3D; FALSE)### Apply ARM rulesrules &lt;- arules::apriori(arm, parameter &#x3D; list(supp&#x3D;0.01, conf&#x3D;0.5, minlen&#x3D;2))# 15 Supportsupport &lt;- sort(rules, by &#x3D; &quot;support&quot;, decreasing &#x3D; TRUE)inspect(support[1:15])subrules &lt;- head(sort(support,by&#x3D;&quot;support&quot;),15)plot(subrules,method&#x3D;&quot;graph&quot;,engine &#x3D;&quot;interactive&quot;)# 15 Confidenceconfidence &lt;- sort(rules, by &#x3D; &quot;confidence&quot;, decreasing &#x3D; TRUE)inspect(confidence[1:15])subrules1 &lt;- head(sort(confidence,by&#x3D;&quot;confidence&quot;, decreasing &#x3D; TRUE),15)plot(subrules1,method&#x3D;&quot;graph&quot;,engine &#x3D;&quot;interactive&quot;)#15 Liftlift &lt;- sort(rules, by &#x3D; &quot;lift&quot;, decreasing &#x3D; TRUE)inspect(lift[1:15])subrules2 &lt;- head(sort(lift,by&#x3D;&quot;lift&quot;),15)plot(subrules2,method&#x3D;&quot;graph&quot;,engine &#x3D;&quot;interactive&quot;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Association Rule Mining </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unsupervised Machine Learning </tag>
            
            <tag> Association Rule </tag>
            
            <tag> Code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Association Rule Mining Data Preparation</title>
      <link href="/2024/02/20/arm-data/"/>
      <url>/2024/02/20/arm-data/</url>
      
        <content type="html"><![CDATA[<h1 id="ARM-Data-Preparation"><a href="#ARM-Data-Preparation" class="headerlink" title="ARM Data Preparation"></a>ARM Data Preparation</h1><p>First, calculate the saving goals rate(<code>rate_saved</code>) from clean_laliga_playerDF, and select variables of <code>season</code>, <code>teamName</code>, <code>position</code>, <code>game_minutes</code>, <code>rate_saved</code>, and <code>rating</code> to generate a new dataframe. Second, calculate the team’s points for each season(<code>PTS</code>) from the clean_laliga_teamDF CSV file and extract the columns of <code>season</code>, <code>teamName</code>, and <code>PTS</code> to generate a new dataframe. Then, using these two new datafram and merge them with teamName and season as ID. Next, the data of selecting only the goalkeeper’s position and picking the goalkeeper’s playing time is greater than 95 minutes (90 minutes is 1 game, so less than 95 has no reference significance, and it may directly exclude). After that, generating a new dataframe prepare for the next step. (As shown in the following figure, the first stage of data preparation is done) </p><p><img src="/ARM/data/1.jpg" alt="First Stage of Data Prepartion"></p><p>By following the above, based on the first stage, select variables of <code>game_minutes</code>, <code>rating</code>, <code>rate_saved</code>, and <code>PTS</code> to generate a new dataframe. Since these data are numeric data, so they need to be converted to transaction data.</p><p>The conversion rules are as follows:</p><p><img src="/ARM/data/2.JPEG" alt="Coversion Rules"></p><p>Converting the dataset to transaction data by following the rules above. Saving this dataframe in CSV format for exploring association rules.</p><p><img src="/ARM/data/3.jpg" alt="Transcation Data"></p><hr><h1 id="Resource"><a href="#Resource" class="headerlink" title="Resource:"></a>Resource:</h1><p><em><strong>Data Paparation Code<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/ARM/ARM_Data_Prep.R">R Code</a></strong></em><br><em><strong>Transcation Data<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/ARM/arm.csv">CSV File</a></strong></em></p>]]></content>
      
      
      <categories>
          
          <category> Association Rule Mining </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Preparation </tag>
            
            <tag> Association Rule Mining </tag>
            
            <tag> Unsupervised Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Association Rule Mininig Overview</title>
      <link href="/2024/02/20/arm-intro/"/>
      <url>/2024/02/20/arm-intro/</url>
      
        <content type="html"><![CDATA[<h1 id="Association-Rule-Mining"><a href="#Association-Rule-Mining" class="headerlink" title="Association Rule Mining"></a>Association Rule Mining</h1><p>Association Rule Mining is an unsupervised learning. The ARM algorithm can discover the relationship between items in the transaction dataset, and it can be applied to multiple scenarios in the real world. The best known example is shopping analytics, which digs into consumers’ transaction records to find correlations between products. As a result, it improves sales volume by recommending relevant products to customers. Therefore, ARM is an important and useful data analysis technique.</p><h1 id="The-Rules"><a href="#The-Rules" class="headerlink" title="The Rules"></a>The Rules</h1><p>The association rule represents the pattern found in the dataset. It can be represented by $X \to Y$. (It can be expressed using an if-then literal expression.). Here X is called <code>antecedent/left-hand-side</code> and Y is called <code>consequent/right-hand-side</code>. There are three commonly used metrics when Using ARM, which are <code>Support</code>, <code>confidence</code>, and <code>lift</code>. They can be used to quantify the association rules between things, so as to reflect the usefulness of the rules.</p><p><img src="/ARM/overview/1.png" alt="Association Rules"></p><p>Here is an example transaction table plot below, to prepare for introducing the support, confidence, and lift.</p><p><img src="/ARM/overview/2.png" alt="An Transaction Table Example"></p><h2 id="Support"><a href="#Support" class="headerlink" title="Support"></a>Support</h2><p>Assume a rule $Rice \to Chicken$ based on the figure above. $Support(Rice, Chicken)$ means how often items in $Rice$ and items in $Chicken$ occur together relative to all transactions. In other words, it represents the ratio of the number of product combinations of occurrences to the total number of occurrences.</p><p>$$Support &#x3D; \frac{frq(Rice, Chicken)}{N} &#x3D; \frac{2}{8} &#x3D; 0.25$$</p><h2 id="Confidence"><a href="#Confidence" class="headerlink" title="Confidence"></a>Confidence</h2><p>Using the same rule above $Rice \to Chicken$. $Confidence(Rice, Chicken)$ means how often items in $Rice$ and items in $Chicken$ occur together relative to transactions that contain Rice. In other words, it represents the probability that when a user buys Rice, the user will purchase Chicken. </p><p>$$Confidence &#x3D; \frac{frq(Rice, Chicken)}{frq(Rice)} &#x3D; \frac{2}{4} &#x3D; 0.5$$</p><h2 id="Lift"><a href="#Lift" class="headerlink" title="Lift"></a>Lift</h2><p>Still Using the same rule above $Rice \to Chicken$, $Lift(Rice, Chicken)$ means the extent to which the appearance of $Rice$ increases the appearance probability of $Chicken$.</p><p>$$Lift &#x3D; \frac{Support(Rice,Chicken)}{Support(Rice) * Support(Chicken)} &#x3D; \frac{0.25}{0.5 * 0.25} &#x3D; 2 $$</p><h1 id="Apriori-Algorithm"><a href="#Apriori-Algorithm" class="headerlink" title="Apriori Algorithm"></a>Apriori Algorithm</h1><p>Apriori Algorithm is one of the most common algorithms used by association rule mining to find frequent itemsets. These frequent itemsets help data analysts make some decisions. Using Support, Confidence, or Lift to evaluate itemsets. The goal of Apriori is to find the maximum set of K frequent itemsets. Here is an example below, using Support as a criterion for evaluating frequent itemsets. After that, Apriori uses an iterative method to find the candidate itemset and its corresponding support and drop out item set with lower support to obtain frequent itemset and so on.</p><p><img src="/ARM/overview/3.png" alt="How Apriori Algorithm Work"></p><h1 id="Plan-For-The-Project"><a href="#Plan-For-The-Project" class="headerlink" title="Plan For The Project"></a>Plan For The Project</h1><p>Focusing on exploring the impact of a goalkeeper’s save ability on a club’s ranking. Selecting the features of conceding goals, the ratio of goals saving, the game minutes, performance rating, and the club ranking, then using the ARM to study the correlation between the goalkeeper’s saving ability and the team’s ranking. Also, setting minimum support, confidence and other parameters ensures the reliability of the association rules have been discovered. Finally, using frequent itemset analyze the relationship between goalkeepers ‘saving ability and clubs’ ranking.</p><hr><h1 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h1><p>Image 1: <a href="https://images.app.goo.gl/88PuEgtbmvfMNzXW7">https://images.app.goo.gl/88PuEgtbmvfMNzXW7</a></p><p>Image 2: <a href="https://images.app.goo.gl/8KXKYytYNBonLcz78">https://images.app.goo.gl/8KXKYytYNBonLcz78</a></p><p>Image 3: <a href="https://www.cnblogs.com/pinard/p/6293298.html">https://www.cnblogs.com/pinard/p/6293298.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Association Rule Mining </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Association Rule Mining </tag>
            
            <tag> Unsupervised Machine Learning </tag>
            
            <tag> Overview </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clustring Conclusion</title>
      <link href="/2024/02/20/clust-conclu/"/>
      <url>/2024/02/20/clust-conclu/</url>
      
        <content type="html"><![CDATA[<p><img src="/clustring/results/7.jpeg" alt="La Liga Clubs"></p><p>All in all, there exists a relationship between the quality of a player’s shooting and a club’s ranking, but the relationship is not conclusive. The number of shots, the number of shots on, and goals scored by players can sometimes affect the outcome and ranking of a club, but this effect is only a small part of the factors. Thus, no matter how perfect a player’s shooting quality is, it does not mean that his team will achieve better results. For example, Celta’s Iago Aspas has goal-scoring and shot quality well in the 2020-2021 season, but his team is still in the mid-table. Therefore, It cannot directly judge the team’s results by quality shooting factors.</p><hr><h1 id="Resouce"><a href="#Resouce" class="headerlink" title="Resouce:"></a>Resouce:</h1><p>image: <a href="https://images.app.goo.gl/dA3Yzg3yB7nDaxeN9">https://images.app.goo.gl/dA3Yzg3yB7nDaxeN9</a></p>]]></content>
      
      
      <categories>
          
          <category> Clutersing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Conclusion </tag>
            
            <tag> Unsupervised Machine Learning </tag>
            
            <tag> Clustring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clustring Results</title>
      <link href="/2024/02/20/clust-result/"/>
      <url>/2024/02/20/clust-result/</url>
      
        <content type="html"><![CDATA[<h1 id="K-Means-Clustering"><a href="#K-Means-Clustering" class="headerlink" title="K-Means Clustering"></a>K-Means Clustering</h1><p>Before the k-means analysis, the appropriate K value needs to be selected. The methods used here are the Elbow method and the Silhouette method and picking two k values by them. Also, choosing another k value randomly. After that, using these k values to process k-means clustering and show the results.</p><p><img src="/clustring/results/1.png" alt="Elbow and Silhouette Methods - K Values"></p><p>According to the optimal k-value plot above, the optimal k-value of the Elbow method is <strong>3</strong> and the optimal k-value of the Silhouette method is <strong>2</strong>.</p><p>Therefore, Using k-values <code>2</code>, <code>3</code>, and randomly picking k-value <code>4</code> to perform  K-means clustering. </p><p><img src="/clustring/results/2.png" alt="K Means Clustering For Each K-Value"></p><p><img src="/clustring/results/5.png" alt="K Means Clustering Exploring 2020 Season"></p><p>The above is the K-means cluster plot and the plot of the number of each cluster for each club in the 2020 season according to the different K-means plots. Using these two figures to determine the appropriate k value and explore the relationship between the quality of players’ shooting and the team ranking.  The yellow cluster is used here to determine whether the k value is appropriate, and the yellow one represents the players with better shooting quality.</p><p>When K &#x3D; 2, it shows “2020 Season – 2 clusters” in the second plot, it does not do a good job of displaying whether a player’s shooting quality is related to clubs’ ranking&#x2F;wins.  As mentioned above, the yellow cluster represents players with better shooting quality, while Cluster 1 represents players with average-level shooting quality.  Interestingly, the three relegated clubs, Eibar, Valladolid, and Huesca, had more good quality shooting than La Liga Champions Atletico Madrid.  Therefore, processing clustering analysis based on k value 2 may not be a good option.</p><p>The subplot “2020 Season – 4 Cluster” in the second figure may work better when K &#x3D; 4 than K value is 2. However, it still does not do a prefect performance of showing the relationship between a players’ shooting quality and a teams’ ranking&#x2F;win. For example, Barcelona has more players in the yellow cluster than Atletico Madrid, but Barce is ranked lower than Atletico Madrid.</p><p>The subplot “2020 Season – 3 Clusters” in the second plot works better when K &#x3D; 3 than when K &#x3D; 2 and 4. From that subplot, it is clearly obvious that Atletico Madrid has far more people in the yellow cluster than any other club. Thus, the k value is 3 is relatively effective when performing cluster analysis. </p><h1 id="Hierarchical-clustering"><a href="#Hierarchical-clustering" class="headerlink" title="Hierarchical clustering"></a>Hierarchical clustering</h1><p>The following Dendrogram shows the effect of hierarchical clustering using the Cosin Similarity maximum distance calculation and the complete linkage algorithm.</p><p><img src="/clustring/results/3.png" alt="The Original Dendrogram"></p><p>The signs shown in the plot are small and hard to read.  In order to deal with this issue, using current () in R cut the tree structure by height to divide the multiple group.  It is helpful to observe the clustering of the dendrogram.</p><p><img src="/clustring/results/4.png" alt="After cutting a group of the original dendrogam"></p><p>The above plot represents splitting from a height of 0.3 and displays the first group data after the division. After splitting, all of the signs of this group can be clearly seen.   Therefore, this method may be able to study dendrogram in-depth.   The K value cannot be determined directly by the cutoff point(height) on the tree.   For example, according to the original dendrogram, choosing a height of approximately 1.8~1.9 is a good choice.   Thus, the K value is 3 based on this height.</p><h3 id="K-means-and-hclust-comparison"><a href="#K-means-and-hclust-comparison" class="headerlink" title="K-means and hclust comparison"></a>K-means and hclust comparison</h3><p><img src="/clustring/results/6.jpg" alt="Hclust - Cluster(the player with the best shooting quality) When k = 3"></p><p>After applying K-means clustering and hierarchical clustering, there are some differences between them. According to the above content, K-means can use the elbow method, silhouette method, or gap statistics to pick the best K-value, and process k-means clustering based on these K values. After that, analyze whether the player’s shooting quality is related to the clubs’ ranking of each season according to the k-means clustering results. Different from K-means, hierarchical clustering requires using cosine similarity as distance to process clustering. If you want to determine the k value based on hierarchical clustering, you may need to determine the best k value based on the cut point of the tree plot, so that data is divided into different clusters.  For example, in this case, the tree is divided into clusters according to the k &#x3D; 3, and the cluster the player with the best shooting quality. Comparing with the yellow cluster when k &#x3D; 3 in k-means. In K-means, when k &#x3D; 3, the players who appear most in the yellow cluster are Atletico Madrid players, but there is not any player of Atletico Madrid who appears in the hierarchical cluster. Therefore, if the dendrogram is clustered based on the k value, the results may be quite different from the result of K-means.</p>]]></content>
      
      
      <categories>
          
          <category> Clutersing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unsupervised Machine Learning </tag>
            
            <tag> Results </tag>
            
            <tag> Clustring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clustring Code</title>
      <link href="/2024/02/20/clust-code/"/>
      <url>/2024/02/20/clust-code/</url>
      
        <content type="html"><![CDATA[<h1 id="Python-Code-for-K-Means-Clustering"><a href="#Python-Code-for-K-Means-Clustering" class="headerlink" title="Python Code for K-Means Clustering"></a>Python Code for K-Means Clustering</h1><p><em><strong>Here is the K-Means code like: <a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/clustering/K-means.py">Python Code</a></strong></em></p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>cluster <span class="token keyword">import</span> KMeans<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> StandardScaler<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> silhouette_scoreclustering_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">"clustering_analysis_df.csv"</span><span class="token punctuation">)</span><span class="token comment">######################### K-Means Clustering ##############</span>X <span class="token operator">=</span> clustering_df<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'shots_total'</span><span class="token punctuation">,</span><span class="token string">'shots_on'</span><span class="token punctuation">,</span><span class="token string">'goals_total'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>scaler <span class="token operator">=</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>scale_X <span class="token operator">=</span> scaler<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token comment">######################### K Value Selection ###############</span>wcss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>score <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span><span class="token comment">#### Elbow Method #########################################</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  model <span class="token operator">=</span> KMeans<span class="token punctuation">(</span>n_clusters<span class="token operator">=</span>i<span class="token punctuation">)</span>  labels <span class="token operator">=</span> model<span class="token punctuation">.</span>fit_predict<span class="token punctuation">(</span>X<span class="token punctuation">)</span>  wcss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>model<span class="token punctuation">.</span>inertia_<span class="token punctuation">)</span><span class="token comment">########## Silhouette Method #############################</span><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  model <span class="token operator">=</span> KMeans<span class="token punctuation">(</span>n_clusters<span class="token operator">=</span>i<span class="token punctuation">)</span>  labels <span class="token operator">=</span> model<span class="token punctuation">.</span>fit_predict<span class="token punctuation">(</span>X<span class="token punctuation">)</span>  scores <span class="token operator">=</span> silhouette_score<span class="token punctuation">(</span>X<span class="token punctuation">,</span>model<span class="token punctuation">.</span>labels_<span class="token punctuation">)</span>  score<span class="token punctuation">.</span>append<span class="token punctuation">(</span>scores<span class="token punctuation">)</span>fig<span class="token punctuation">,</span> <span class="token punctuation">(</span>ax1<span class="token punctuation">,</span> ax2<span class="token punctuation">)</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>ax1<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">,</span> wcss<span class="token punctuation">,</span> marker<span class="token operator">=</span><span class="token string">'o'</span><span class="token punctuation">)</span>ax1<span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'r'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">)</span>ax1<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'Number of clusters K'</span><span class="token punctuation">)</span>ax1<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'WCSS'</span><span class="token punctuation">)</span>ax1<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'Elbow Method'</span><span class="token punctuation">)</span>ax2<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">)</span><span class="token punctuation">,</span> score<span class="token punctuation">,</span> marker<span class="token operator">=</span><span class="token string">'o'</span><span class="token punctuation">)</span>ax2<span class="token punctuation">.</span>axvline<span class="token punctuation">(</span>x<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'r'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">)</span>ax2<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">'Number of clusters K'</span><span class="token punctuation">)</span>ax2<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">'Silhouette Score'</span><span class="token punctuation">)</span>ax2<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">'Silhouette Method'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">############# K-Mean Visualization ###########################</span>df2 <span class="token operator">=</span> clustering_df<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>model2 <span class="token operator">=</span> KMeans<span class="token punctuation">(</span>n_clusters<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>labels2 <span class="token operator">=</span> model2<span class="token punctuation">.</span>fit_predict<span class="token punctuation">(</span>X<span class="token punctuation">)</span>df2<span class="token punctuation">[</span><span class="token string">"cluster"</span><span class="token punctuation">]</span> <span class="token operator">=</span> labels2df3 <span class="token operator">=</span> clustering_df<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>model3 <span class="token operator">=</span> KMeans<span class="token punctuation">(</span>n_clusters<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>labels3 <span class="token operator">=</span> model3<span class="token punctuation">.</span>fit_predict<span class="token punctuation">(</span>X<span class="token punctuation">)</span>df3<span class="token punctuation">[</span><span class="token string">"cluster"</span><span class="token punctuation">]</span> <span class="token operator">=</span> labels3df4 <span class="token operator">=</span> clustering_df<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>model4 <span class="token operator">=</span> KMeans<span class="token punctuation">(</span>n_clusters<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>labels4 <span class="token operator">=</span> model4<span class="token punctuation">.</span>fit_predict<span class="token punctuation">(</span>X<span class="token punctuation">)</span>df4<span class="token punctuation">[</span><span class="token string">"cluster"</span><span class="token punctuation">]</span> <span class="token operator">=</span> labels4fig<span class="token punctuation">,</span> <span class="token punctuation">(</span>ax1<span class="token punctuation">,</span> ax2<span class="token punctuation">,</span> ax3<span class="token punctuation">)</span> <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>ax1<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>df2<span class="token punctuation">[</span><span class="token string">'shots_total'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>df2<span class="token punctuation">[</span><span class="token string">'shots_on'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c <span class="token operator">=</span> df2<span class="token punctuation">[</span><span class="token string">'cluster'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>ax1<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"Shots Total"</span><span class="token punctuation">)</span>ax1<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">"Shots On"</span><span class="token punctuation">)</span>ax1<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"K Value = 2 &amp; Silhouette Method"</span><span class="token punctuation">)</span>ax2<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>df3<span class="token punctuation">[</span><span class="token string">'shots_total'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>df3<span class="token punctuation">[</span><span class="token string">'shots_on'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c <span class="token operator">=</span> df3<span class="token punctuation">[</span><span class="token string">'cluster'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>ax2<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"Shots Total"</span><span class="token punctuation">)</span>ax2<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">"Shots On"</span><span class="token punctuation">)</span>ax2<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"K Value = 3 &amp; Elbow Method"</span><span class="token punctuation">)</span>ax3<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>df4<span class="token punctuation">[</span><span class="token string">'shots_total'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>df4<span class="token punctuation">[</span><span class="token string">'shots_on'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>c <span class="token operator">=</span> df4<span class="token punctuation">[</span><span class="token string">'cluster'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>ax3<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"Shots Total"</span><span class="token punctuation">)</span>ax3<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">"Shots On"</span><span class="token punctuation">)</span>ax3<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"K Value = 4 &amp; Random Picking"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><h1 id="R-Code-For-hierarchical-clustering"><a href="#R-Code-For-hierarchical-clustering" class="headerlink" title="R Code For hierarchical clustering"></a>R Code For hierarchical clustering</h1><p><em><strong>Here is the Hclust Code: <a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/clustering/hclust.R">R Code</a></strong></em></p><pre class="language-R" data-language="R"><code class="language-R">#install.packages(&quot;stylo&quot;)#install.packages(&quot;factoextra&quot;)library(tidyverse)library(cluster)library(factoextra)library(stylo) ## for using dis cosine# Set the absolute file pathsetwd(&quot;D:&#x2F;clustering&quot;)#Read the csv filehclust_df &lt;- read.csv(&#39;clustering_analysis_df.csv&#39;)#Select shots_total, shots_on, and goals_total variablesdata &lt;- hclust_df[, 5:7]data_scl &lt;- scale(data) # scale datadist_df &lt;- stylo::dist.cosine(data_scl) # compute cosine Cosine Similarityhc &lt;- hclust(dist_df, method &#x3D; &quot;complete&quot;) # using copplot(hc, cex &#x3D; 0.1, hang &#x3D; -1, main &#x3D; &quot;Cosine&quot;,labels &#x3D; hclust_df$playerName)tree_height &lt;-0.3 # cut tree height from the original hclustclus &lt;- cutree(hc, h &#x3D; tree_height)temp &lt;- data[clus &#x3D;&#x3D; 1, ]temp_dist &lt;- stylo::dist.cosine(scale(temp)) temp_hc &lt;- hclust(temp_dist, method &#x3D; &quot;complete&quot;)l &lt;- hclust_df$playerName[clus &#x3D;&#x3D; 1]plot(temp_hc, cex &#x3D; 0.5, hang &#x3D; -30, main &#x3D; &quot;Cosine&quot;, labels &#x3D; l)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Clutersing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unsupervised Machine Learning </tag>
            
            <tag> Code </tag>
            
            <tag> Clustring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clustring Data Preparation</title>
      <link href="/2024/02/20/clust-data/"/>
      <url>/2024/02/20/clust-data/</url>
      
        <content type="html"><![CDATA[<h1 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h1><h2 id="Dataframes-Before-Transforming"><a href="#Dataframes-Before-Transforming" class="headerlink" title="Dataframes Before Transforming"></a>Dataframes Before Transforming</h2><p>Before processing Clustering analysis, it is necessary to extract the data required for Clustering analysis from the previously cleaned datasets (<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/clean_laliga_playerDF.csv">Players Datasets</a> &amp; <a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/clean_laliga_teamDF.csv">Clubs Dataset</a>) and combine the extracted data in a dataframe. It can be used for analysis. The figure below shows what the two datasets looked like before they were extracted and merged. </p><p><img src="/clustring/data/1.jpg" alt="Cleaned La Liga Players Dataset"></p><p><img src="/clustring/data/2.jpg" alt="Cleaned La Liga Clubs Dataset"></p><p>This clustering analysis just focuses on the relationship between players’ shooting quality and the team’s performance(<a href="https://hleof.github.io/2024/02/20/clust-intro/#Clustering-Plan">Clusetring Plan</a>). Therefore, Extracting the variables: <code>teanName</code>, <code>playerName</code>, <code>position</code>, <code>shots_total</code>,<code>shots_on</code>, and <code>goals_total</code> from the player dataset. <strong>The main reason for needing position variables is to drop the goalkeepers’ data because the goalkeepers are not involved in the attack or shooting for a game. Players in this position do not need to be considered in the analysis.</strong> For the clubs’ dataset, after calculating the points for each club per season and generating a new column called PTS(<em><strong>3 Points for a win, 1 Point for a draw, and 0 points for a loss</strong></em>), all extracted data can be combined into a data frame and ready for next step.</p><p><img src="/clustring/data/4.jpg" alt="New Merged DataFrame"></p><h2 id="DataFrame-After-Transforming"><a href="#DataFrame-After-Transforming" class="headerlink" title="DataFrame After Transforming"></a>DataFrame After Transforming</h2><p>According to the above Merge Dataframe, picking shots_total, shots_on, and goal_total as eigenvalue for this clustering analysis. These three variables are both unlabeled numeric data and are suitable for processing clustering analysis. </p><p><img src="/clustring/data/3.jpg" alt="Data For Clustering Analysis"></p><p>After that, it may use following code to transform and scale these data and get ready to implement clustering algorithms.</p><pre class="language-python" data-language="python"><code class="language-python">X <span class="token operator">=</span> clustering_df<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'shots_total'</span><span class="token punctuation">,</span><span class="token string">'shots_on'</span><span class="token punctuation">,</span><span class="token string">'goals_total'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>scaler <span class="token operator">=</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>scale_X <span class="token operator">=</span> scaler<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X<span class="token punctuation">)</span></code></pre><p><img src="/clustring/data/5.jpg" alt="variables tranfrom and scale"></p><p>After finishing the clustering analysis, other variables in the merged dataframe will be used for in-depth explore the relationship between players’ shooting quality and the team’s performance. </p>]]></content>
      
      
      <categories>
          
          <category> Clutersing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Preparation </tag>
            
            <tag> Unsupervised Machine Learning </tag>
            
            <tag> Clustring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Clustring Method Overview</title>
      <link href="/2024/02/20/clust-intro/"/>
      <url>/2024/02/20/clust-intro/</url>
      
        <content type="html"><![CDATA[<h1 id="Clustering-Analysis"><a href="#Clustering-Analysis" class="headerlink" title="Clustering Analysis"></a>Clustering Analysis</h1><p>Clustering is an unsupervised learning. It can divide the objects of analysis into different groups based on their features, in other words dividing the data in the dataset into clusters with similar characteristics. The goal lets data within a category to be more similar than data between different categories. Clustering analysis is different from classification, which is a supervised learning method that requires learning a classification from a training dataset. Clustering analysis is widely used in the real world, it can be used in computer image recognition, medical image analysis, market research, commodity classification, etc.</p><h2 id="Partitioning-Clustering-VS-Hierarchical-Clustering"><a href="#Partitioning-Clustering-VS-Hierarchical-Clustering" class="headerlink" title="Partitioning Clustering VS. Hierarchical Clustering"></a>Partitioning Clustering VS. Hierarchical Clustering</h2><p>Clustering analysis has two types, which are Partitioning clustering and hierarchical clustering. Partitioning clustering is not hierarchical, it has not explicit structure between data to show that they are related to each other. K-Means and K-Medoids are the most common useful partitioning algorithms. Hierarchical clustering shows a hierarchical tree structure where each node represents a cluster. This method gradually merged data (<code>Agglomerative Clustering</code>) or divided (<code>Divisive Clustering</code>) until all the data was clustered together. It does not require the number of clusters (<code>K values</code>) to be specified in advance. Single-linkage, Complete-linkage, and Connectivity-based clustering are the most common algorithms of Hierarchical clustering. The time complexity of partitioning clustering is relatively low compared to the hierarchical distance, especially in the case of large data volume, because hierarchical clustering needs to consider the distance or similarity between the data for each step.</p><p><img src="/clustring/overview/1.png" alt="Partitioning Clustering Example"></p><p><img src="/clustring/overview/2.png" alt="Hierarchical Clustering Example"></p><p>The plots above show the difference between the two categories of clustering. The Euclidean distance is used here, which is calculated straight-line distance between two points. The advantage of the Euclidean method is that is relatively simple to implement and intuitive. If a dataset is a low-dimensional dataset, the effect of using it is perfect. The dataset used in these two plots is low-dimensional(dataset).</p><p>$$\text{Euclidean Distance} &#x3D; d(x,y) &#x3D; \sqrt{\sum_{i&#x3D;1}^{n} (x_{i}-y_{i})^2} $$</p><p>The most obvious difference between these two plots, as mentioned above, is that they show different structures. Partitioning clustering does not have a clear structure, and it can only be distinguished clusters by color and space.  The plot of hierarchical clustering can show the division of clusters by tree structure. Using the K &#x3D; 3 to display clusters, the partitioning and hierarchical plots have different amounts of data in each cluster. When using clustering analysis, may selecting different categories of clustering meets needs. For example, the partitioning method is a good choice when analyzing large-scale datasets, it is often more efficient and faster to perform. The hierarchical method is an option when exploring the structure of data clustering. It is suitable for working with small or medium-scale datasets, but it is expensive to work with large-scale datasets.</p><h1 id="Clustering-Plan"><a href="#Clustering-Plan" class="headerlink" title="Clustering Plan"></a>Clustering Plan</h1><p>Finding the relationship between the shooting quality efficiency of players in La Liga and the results of matches is explored by using clustering analysis. Selecting indexes include the total number of shots, the number of shots, the total number of goals, and the total number, which are important features to evaluate the quality and efficiency of the players’ shots.</p><p>Using K-mean algorithms and hierarchical clustering method to cluster the shooting indicators of players. Through these two algorithms, they are possible to find differences in shooting performance between different groups and explore whether shooting performance between different levels of players is correlated with match results and club rankings. It is super helpful to get a complete understanding of the impact of shooting quality on a club’s results. At the same time, compare the clustering effect produced by K-means and hierarchical clustering algorithm respectively to determine which method is more suitable for this project.</p><hr><h1 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h1><p><em><strong>Example Dataset<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/clustering/example/text.csv">Dataset</a></strong></em></p><p><em><strong>Example K-Means Python Code<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/clustering/example/clustring.py">K-Means Code</a></strong></em></p><p><em><strong>Example Hierarchical Clustring R Code<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/clustering/example/hclust.R">Hierarchical Clustring Code</a></strong></em></p>]]></content>
      
      
      <categories>
          
          <category> Clutersing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unsupervised Machine Learning </tag>
            
            <tag> Clustring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Gathering, Cleaning, and Preparation</title>
      <link href="/2024/02/04/DataPrep/"/>
      <url>/2024/02/04/DataPrep/</url>
      
        <content type="html"><![CDATA[<h1 id="Gathering-Data"><a href="#Gathering-Data" class="headerlink" title="Gathering Data"></a>Gathering Data</h1><p>Some datasets available on the website are suitable for meeting needs (either a single season or only up to 2020). Therefore, in order to get the latest seasons from 2020-2023, just using APIs to gather data and create datasets is a good option. Finding an API calls <code>api-football</code> from the <code>rapidapi&#39;</code>‘s website and using it to get all the data. This API provides rich data of La Liga players and clubs. Obtaining data has wonderful acquisition and accuracy. After gathering data from this API, create two unique datasets: one for players’s data, and another one for clubs’ data.</p><h2 id="API-Sample-Code"><a href="#API-Sample-Code" class="headerlink" title="API Sample Code"></a>API Sample Code</h2><ol><li><p>The example code below shows the API to get the various data of FC Barcelona’s players in La Liga from 2022-2023. Using this API to get all player’s data for La Liga from 2020-2023. </p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> requests<span class="token comment">#endpoint</span>url <span class="token operator">=</span> <span class="token string">"https://api-football-v1.p.rapidapi.com/v3/players"</span><span class="token comment">#to find the players'data in FC Barcelona 2020</span><span class="token comment"># 529 means Barcelona</span><span class="token comment"># 140 means Laliga</span><span class="token comment"># season means 2020 year</span>query <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"team"</span><span class="token punctuation">:</span><span class="token string">"529"</span><span class="token punctuation">,</span><span class="token string">"league"</span><span class="token punctuation">:</span><span class="token string">"140"</span><span class="token punctuation">,</span><span class="token string">"season"</span><span class="token punctuation">:</span><span class="token string">"2020"</span><span class="token punctuation">&#125;</span>headers <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">"X-RapidAPI-Key"</span><span class="token punctuation">:</span> Your_key<span class="token punctuation">,</span> <span class="token comment"># Your private API key</span>    <span class="token string">"X-RapidAPI-Host"</span><span class="token punctuation">:</span> <span class="token string">"api-football-v1.p.rapidapi.com"</span><span class="token punctuation">&#125;</span>response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">,</span> params<span class="token operator">=</span>query<span class="token punctuation">)</span>response<span class="token punctuation">.</span>text</code></pre><p> Run code, it shows the players’ data of FC Barcelona(click picture for <strong>zoom in</strong>)<br> <img src="/dataPrep/1.jpg" alt="Players Data From FC Barcelona (JSON)"></p></li><li><p>The example code below shows the API get various data of Barcelona (include wins, loses, draws, goal for, goal against, etc for each season) for La Liga from 2020-2023.</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> requestsurl <span class="token operator">=</span> <span class="token string">"https://api-football-v1.p.rapidapi.com/v3/teams/statistics"</span><span class="token comment"># Getting the data of FC Barcelona in 2020</span><span class="token comment"># 529 means Barcelona</span><span class="token comment"># 140 means Laliga</span><span class="token comment"># season means 2020 year</span>query <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token string">"league"</span><span class="token punctuation">:</span><span class="token string">"140"</span><span class="token punctuation">,</span><span class="token string">"season"</span><span class="token punctuation">:</span><span class="token string">"2020"</span><span class="token punctuation">,</span><span class="token string">"team"</span><span class="token punctuation">:</span><span class="token string">"529"</span><span class="token punctuation">&#125;</span>headers <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">"X-RapidAPI-Key"</span><span class="token punctuation">:</span> Your_key<span class="token punctuation">,</span>    <span class="token string">"X-RapidAPI-Host"</span><span class="token punctuation">:</span> <span class="token string">"api-football-v1.p.rapidapi.com"</span><span class="token punctuation">&#125;</span>response <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">,</span> params<span class="token operator">=</span>query<span class="token punctuation">)</span>response<span class="token punctuation">.</span>text</code></pre><p> Run this sample code, it show the Barcelona’s Data in 2020 (click picture for <strong>zoom in</strong>)<br> <img src="/dataPrep/2.jpg" alt=" Data of FC Barcelona in 2020 (JSON)"></p></li></ol><h2 id="Getting-Raw-Data"><a href="#Getting-Raw-Data" class="headerlink" title="Getting Raw Data"></a>Getting Raw Data</h2><p>Getting the raw data from these two APIs.<br><img src="/dataPrep/3.jpg" alt="Raw Dataset of Players"></p><p>The players’ raw data have the amount of missing values, Nan, and incorrect values. Therefore, for the next step, pick a suitable way to clean&#x2F;replace these values and get a clean player’s dataset.</p><p><img src="/dataPrep/6.jpg" alt="Raw Dataset of Clubs"></p><p>Also, for the next step, check these two data frames data types, changing them as well to get a better analysis process and results.</p><p><em><strong>Here is Full Version Code for gathering data:<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/api_garthering_data.py">API Code</a></strong></em><br><em><strong>Full Raw Dataset of Players: <a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/raw_laliga_playerDF.csv">Raw Dataset of Plyasers</a></strong></em><br><em><strong>Full Raw Dataset of Clubs: <a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/raw_laliga_teamDF.csv">Raw Dataset of Clubs</a></strong></em></p><hr><h1 id="Cleaning-Data-Visualization"><a href="#Cleaning-Data-Visualization" class="headerlink" title="Cleaning Data &amp; Visualization"></a>Cleaning Data &amp; Visualization</h1><h2 id="Cleaning"><a href="#Cleaning" class="headerlink" title="Cleaning"></a>Cleaning</h2><p>For the data cleaning step, the first thing is to remove some of the attributes that are not useful. At the same time, use <code>countbox</code> plots to check whether the values of those columns are all the same or all blank. The picture below shows variables of <code>injured</code>, <code>number</code>, <code>captain</code>, <code>game_minutes</code>, <code>penalty_won</code>, and <code>dribble_past</code> meet it, so they should be dropped. The <code>teamId</code> and <code>birthDate</code> are also not useful, they also should be removed. For the data club data, just drop the variable of <code>teamID</code>. Therefore, removing these attributes does not affect future tasks. The dataset of clubs almost perfect, it does not some NaN value, missing values, or incorrect values. Thus, dataset of clubs is clean and ready to use.</p><p><img src="/dataPrep/visual/1.png" alt="Check Arrtibute"></p><p>After that, searching and replacing the NaN in datasets. First, the player’s height and weight contain some NaN, and the quantity of them is small, so replacing these NaN values with height means and weight means. Other variables contain a lot of NaN data, such <code>game_assist</code>, <code>goal_saved</code>, <code>game_appearc</code>, etc. Because NaN is these columns mean 0, they also mean that players are not performing in these skills. Therefore, using 0 to replace NaN for these attributes is a good choice.</p><p>Last, changing some data types for the dataset, such as switching height and weight data types from string to integer. Also changing position data type from string to category.</p><p><img src="/dataPrep/5.jpg" alt="Clean Dataset of Players"></p><p><img src="/dataPrep/4.jpg" alt="Clean Dataset of Clubs"></p><p><em><strong>Here is Full Version Code for Cleaning Data<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/data_cleaning.py">Cleaning Data Code</a></strong></em><br><em><strong>Full Clean Dataset of Players<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/clean_laliga_playerDF.csv">Clean Dataset of Players</a></strong></em><br><em><strong>Full Clean Dataset of Clubs<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/clean_laliga_teamDF.csv">Clean Dataset of Clubs</a></strong></em></p><hr><h2 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h2><p><img src="/dataPrep/visual/2.png" alt="Number of Nationality in La Liga"><br>The plot shows in addition to Spanish players, Argentine, Brazilian, French, and Portuguese players play more in La Liga.</p><p><img src="/dataPrep/visual/3.png" alt="Number of Red Card fro each Club 2020-2023"><br>This plot shows the total number of red cards received by the various clubs in La Liga from 2020 to 2023. Clubs getting more red cards may indicate having poor results. Teams that do not get red cards may show a less aggressive style of play. </p><p><img src="/dataPrep/visual/4.png" alt="number of Yellow Card For each Club 2020-2023"><br>This plot shows the total number of yellow cards received by various clubs in La Liga from 2020 to 2023. The number of yellow cards received may indicate a more aggressive style of play. For example, Getafe, this team style of play is very unappealing and they get good results through a lot of fouls.</p><p><img src="/dataPrep/visual/5.png" alt="Player Age and Total Goal Relationship"><br>The age of players from 25 to 30 may be are golden age, they can score more goals.</p><p><img src="/dataPrep/visual/6.png" alt="Number of Goal Save For each Club 2020-2023"><br>A team with more saves may indicate an inconsistent defense and may have a poor performance.</p><p><img src="/dataPrep/visual/7.png" alt="Total Wins For Clubs 2020-2023"><br>The top three teams in La Liga(Barcelona, Real Madrid, and Atletico Madrid) have won more games than any other team, which shows that they are stronger than any other team in La Liga.</p><p><img src="/dataPrep/visual/8.png" alt="Box Plot for Socred Looses For Clubs From 2020 to 2023"><br>The total scored at home by each team in La Liga is generally between 50 to 110 from 2020 to 2023.</p><p><img src="/dataPrep/visual/9.png" alt="Box Plot for Total Losses For Clubs From 2020 to 2023"><br>The total losses at home by each team in La Liga is generally between 50 to 70 from 2020 to 2023.</p><p><img src="/dataPrep/visual/10.png" alt="Total Losses For Clubs From 2020 To 2023"><br>The line plot shows that Real Madrid plays the most consistently each season. Because they have fewer losses every season.</p><p><em><strong>Here is Full Version Code for Data Visualization<a href="https://github.com/HLeoF/laliga_machine_learning/blob/main/laligaProject/viusal.py">Data Visualization Code</a></strong></em></p><hr><h1 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h1><p><code>Background &amp; Cover Image</code>:<a href="https://images.app.goo.gl/rrFyz3J2dLG91KgBA">https://images.app.goo.gl/rrFyz3J2dLG91KgBA</a><br><code>API for Gathering Data</code>: <a href="https://rapidapi.com/api-sports/api/api-football/">https://rapidapi.com/api-sports/api/api-football/</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Data Gathering </tag>
            
            <tag> Data Cleaning </tag>
            
            <tag> Data Preparation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction</title>
      <link href="/2024/01/17/Introduction/"/>
      <url>/2024/01/17/Introduction/</url>
      
        <content type="html"><![CDATA[<h1 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h1><p><em><strong>Is the individual ability of La Liga players a key factor in determining how a club goes throughout a season? Whether a player’s performance on the pitch (such as dribbling, shots, passes, yellow card, red card, etc.) can indicate the outcome of every game and affect a club’s position at the end of each season.</strong></em></p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>&emsp;In real life, a lot of analysis work has been done based on players’ statistics. For example, it is possible to access a player’s performance and potential by analyzing a player’s passing, shooting, dribbling, and other data. This way will support clubs in making more informed choices in the transfer market for each season. Also, players’ data is used to optimize the teams’ training sessions to help players improve their individual abilities and overall performance. More in-depth analysis of player data is now needed to better understand the role and impact of players in games. This word cannot only offer the team more tactical options but also guide the club to make more good decisions during training and games. At the same time, it is desirable to use data analysis to predict fluctuations in players’ states and their performance at critical moments so that improving the stability and combat effectiveness of the teams in a game. Therefore, through in-depth analysis of player data, the club can be provided with tactical options and decision-making support, thereby improving team performance and results during a season. </p><p>&emsp;Soccer is the number one sport in the world, with tens of thousands of soccer fans watching every weekend around the world. Many people like the fast pace and physicality of the Premier League, some like the defensive art of Serie A, and some people like the attacking football of the Bundesliga. According to the latest World Soccer League ranking (<a href="https://www.teamform.com/en/league-ranking/world/2024/q1">https://www.teamform.com/en/league-ranking/world/2024/q1</a>), La Liga is second in the world, and the main reason it has a large group fans&#x2F;audience is that La Liga is known for its ball control and exquisite technique. On the pitch of La Liga, each team’s 11 players operate like a sophisticated machine for 90 minutes. Each man on the pitch is a part of the machine, and their performance affects the operation of the entire machine. Therefore, the various clubs in La Liga pay great attention to the comprehensive quality of players. Every touch, every pass, every breakthrough of players on the pitch is silently writing the fate of a club’s entire season. Thus, just by standing on the pitch they are playing a key role in the success of a club. In the data era, the soccer area is like a tiger that has grown wings. A player’s individual stats will become what the team or scout focuses on, which is an essential factor in the success of the club’s strategic development and team management. In addition, it also provides fans with a deeper soccer-watching experience, making each player’s performance stand out to the audience. </p><p><img src="/introduction/1.jpg" alt="The Latest World Soccer League Ranking"></p><p>&emsp;Many people or fans may subconsciously believe that if a club purchases players who have wonderful individual abilities, the whole club will win all the trophies. However, it is not like FIFA or Football Manager games where players can win everything if they are good enough. One of the more famous examples, Real Madrid bought a large number of expensive players(<a href="https://www.goal.com/en/news/what-galactico-ronaldo-beckham-all-real-madrids-super-signings/1lrrb6x9j0n9f1actzefqw47am">https://www.goal.com/en/news/what-galactico-ronaldo-beckham-all-real-madrids-super-signings/1lrrb6x9j0n9f1actzefqw47am</a>) in 2000 and formed the Galacticos I era(<a href="https://en.wikipedia.org/wiki/Gal%C3%A1cticos#First_gal%C3%A1ctico_era_2">https://en.wikipedia.org/wiki/Gal%C3%A1cticos#First_gal%C3%A1ctico_era_2</a>). At first, as the fans hoped, Real Madrid was able to win various titles from 2000 to 2002. However, since 2002, they have been unable to win any championships just like the honeymoon period of lovers is over. During this period, Real Madrid’s bitter rivals Barcelona won the championships in successive competitions. Therefore, a successful club may not often depend on the outstanding performances of individual players. If the whole club is not well coordinated or appears internal conflicts, the club’s results may also be affected. Besides, a player’s state fluctuation, injuries, and psychological quality are also important factors that affect the results of a game. Even players with outstanding individual skills can hurt the performance of the team once they are out of form or injured.</p><p><img src="/introduction/2.jpg" alt="The Players of Galacticos I Era"></p><p><img src="/introduction/3.jpg" alt="Japan vs. Belgium World Cup 2018"></p><p>&emsp;With the advent of the data era, both big-scale soccer clubs and small-scale clubs are beginning to pay more attention to players’ statistics. Each team has built its data analytics team. Not only it is used to analyze the data of one’s own players to offer a better strategy for the coach, but it is also useful to analyze the data of players of the opposing team to determine their weaknesses and obtain an advantage in a game. For example, before an important Derby game, one club’s data analyst team finds that the opponent’s back line is not strong enough to defend against high-altitude balls. They discovered this weakness through analyzing data such as opposition players’ height and their ability to head the ball. (2018 World Cup, Japan vs. Belgium, Belgium’s national team used this strategy to defeat Japan. The final is 3-2. Japan had the lead for a while.) During the game, the team used frequent crosses, made targeted attacks on the opponent’s weak points, and eventually won the game. In another example, one team’s analysts found that opponents’ midfielders had problems with passing accuracy and were prone to turnovers. They figure out this issue by analyzing the passing success of opposing players and their performance under high pressure. The team uses a high position and force-pressing approach in the game, taking advantage of the loose pass in the opponent’s midfield to launch a quick counterattack, and win the game eventually. Therefore, by making full use of players’ data for opponent analysis, one’s team can better make tactics and strategies, and grasp the shortcomings of opponents, which can help them win the game.</p><p>&emsp;In La Liga, each player demonstrates unique individual abilities that may have a direct impact on the club’s performance over all seasons. Every movement they make on the pitch is like weaving the parts of a precision machine that determines the fate of an entire team. Outstanding performances from individual players alone may not ensure the club’s success, as the club’s overall coherence and internal unity are equally crucial. With the advent of the data age, teams are increasingly relying on data analytics to optimize tactics and strategies in an effort to gain an edge in games. By analyzing players’ data in-depth, clubs can provide more tactical options and decision support to improve their teams’ performance. This journey into the world of soccer data not only brings a new way of managing clubs, but also provides fans with a more in-depth soccer-watching experience. In the future, with the further development of technology and data analytics, exploring full of infinite possibilities in the world of soccer and create more opportunities and fun for players and soccer fans. Now please follow data footsteps and dive into the exciting world behind La Liga through player statistics.</p><h1 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h1><ul><li><p><strong>Whether the official rating of a player can verify the situation of a team (good or bad) and its future trend?</strong></p></li><li><p><strong>Does the quality of a player’s shooting determine the outcome of a game and a club’s trend at the end of a season?</strong> </p></li><li><p><strong>Does the goalkeeper’s save ability become the key to the team’s victory?</strong></p></li><li><p><strong>Whether the number of red and yellow cards obtained by players can affect the trend of the team’s season?</strong></p></li><li><p><strong>Striker, Midfielder, or defender, which position has the most direct impact on the team ranking for each season?</strong></p></li><li><p><strong>Are the players’ on-field technique values (such as passes, defenses, saves, goals, etc.) have a positive correlation with team ranking?</strong></p></li><li><p><strong>Does the quantity and quality of penalty kicks determine a team’s performance?</strong></p></li><li><p><strong>Can a player’s duels won rate determine the outcome of the game?</strong></p></li><li><p><strong>Can players bring better results to the team as their skill boosts as the season progresses?</strong></p></li><li><p><strong>Can the number of lineups and appearances of a player determine a game’s result?</strong></p></li></ul><hr><h1 id="Resource"><a href="#Resource" class="headerlink" title="Resource"></a>Resource</h1><p><code>cover image</code>: <a href="https://images.app.goo.gl/RR4agxTTYDRBukKE8">https://images.app.goo.gl/RR4agxTTYDRBukKE8</a><br><code>World Cup 2018 Japan vs. Belgium</code>:<a href="https://images.app.goo.gl/qfCThxKCB2vMR4Mi6">https://images.app.goo.gl/qfCThxKCB2vMR4Mi6</a></p>]]></content>
      
      
      <categories>
          
          <category> Introduction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> La Liga </tag>
            
            <tag> Introduction </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
